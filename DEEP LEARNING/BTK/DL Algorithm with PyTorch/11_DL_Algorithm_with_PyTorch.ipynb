{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This project is a classification study. The dataset include positive & negative comments. We create it.\n",
        "\n",
        "In the below, there is a basic pipeline/workflow of DL project."
      ],
      "metadata": {
        "id": "UULzpOrGSCNd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nms5XficNbq_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn ## the most important module\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import string\n",
        "from collections import Counter # to find unique value\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Identification and Pre-Processing"
      ],
      "metadata": {
        "id": "bOasuPL1SagG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We create dataset from gpt-5. Prompt:\n",
        "\n",
        "1) Role\n",
        "You are a data generation assistant specialized in creating balanced, clean, and safe text datasets for sentiment analysis.\n",
        "\n",
        "2) Task\n",
        "Generate two Python lists of English sentences:\n",
        "positive_sentences: exactly 100 unique positive opinions/reviews.\n",
        "negative_sentences: exactly 100 unique negative opinions/reviews.\n",
        "\n",
        "3) Context\n",
        "These sentences will be used directly in Python for training and testing sentiment models.\n",
        "Topics should be diverse: products, apps, restaurants, movies/series, customer support, delivery, travel, education, workplace, events, services, and day-to-day experiences.\n",
        "Keep sentences human-sounding and varied in length and style (short, medium, long; simple and compound sentences).\n",
        "Keep content safe and professional: no profanity, slurs, harassment, graphic content, medical or legal claims, or personally identifiable information.\n",
        "Avoid brand names, celebrity names, and sensitive topics (politics, religion).\n",
        "Do not use sarcasm or ambiguous sentiment; each sentence must be clearly positive or clearly negative.\n",
        "No near-duplicates or templated repeats; vary vocabulary and phrasing.\n",
        "Use plain ASCII characters and standard quotes; escape internal quotes correctly.\n",
        "Examples (just examples, do not reuse):\n",
        "Positive: \"This exceeded my expectations in every way.\"\n",
        "Negative: \"The update made everything slower and frustrating.\"\n",
        "\n",
        "4) Reasoning Instruction\n",
        "Before writing the final output, silently plan (chain-of-thought hidden) how to:\n",
        "ensure exactly 100 unique items per list,\n",
        "cover a wide range of everyday domains,\n",
        "maintain clear sentiment and linguistic variety,\n",
        "avoid unsafe content and trademarks,\n",
        "verify Python syntax correctness and proper escaping.\n",
        "Then produce the final answer.\n",
        "\n",
        "5) Output Format\n",
        "Return only a single Python code block with:\n",
        "Two lists named exactly positive_sentences and negative_sentences.\n",
        "Each list contains exactly 100 string elements.\n",
        "One sentence per element, no trailing commas after the last element.\n",
        "No comments, no extra text, no print statements, no explanation outside the code block.\n",
        "Template:\n",
        "positive_sentences = [\n",
        "    \"…\",\n",
        "    # 99 more\n",
        "]\n",
        "\n",
        "negative_sentences = [\n",
        "    \"…\",\n",
        "    # 99 more\n",
        "]\n",
        "\n",
        "6) Stop Conditions\n",
        "Stop immediately after the closing bracket of negative_sentences.\n",
        "Do not include anything outside the Python code block.\n",
        "If you cannot meet any constraint, produce nothing.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hkCbjfxvSflW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_sentences = [\n",
        "    \"This exceeded my expectations in every way.\",\n",
        "    \"The staff was incredibly kind and helpful throughout my visit.\",\n",
        "    \"I love how easy this app is to use on a daily basis.\",\n",
        "    \"The course material was well-structured and very informative.\",\n",
        "    \"Dinner was absolutely delicious, with fresh ingredients and rich flavors.\",\n",
        "    \"The movie had a touching story and brilliant performances.\",\n",
        "    \"Delivery arrived earlier than expected and everything was perfect.\",\n",
        "    \"I enjoyed every moment of the concert, the energy was amazing.\",\n",
        "    \"The workplace atmosphere is supportive and inspiring.\",\n",
        "    \"I appreciate how quickly customer support resolved my issue.\",\n",
        "    \"The hotel room was spotless and had a beautiful view.\",\n",
        "    \"This laptop runs smoothly and handles all my tasks efficiently.\",\n",
        "    \"The tutorial made learning much easier than I anticipated.\",\n",
        "    \"I felt very welcome at the event and met great people.\",\n",
        "    \"The design is elegant and practical at the same time.\",\n",
        "    \"Traveling with this agency was stress-free and enjoyable.\",\n",
        "    \"The workshop provided valuable skills I can use immediately.\",\n",
        "    \"Every staff member went out of their way to make me comfortable.\",\n",
        "    \"The performance was breathtaking and unforgettable.\",\n",
        "    \"I found the book to be insightful and beautifully written.\",\n",
        "    \"This phone’s camera takes sharp and vibrant photos.\",\n",
        "    \"The ride was smooth and the driver was very polite.\",\n",
        "    \"I was impressed by the quick installation and setup process.\",\n",
        "    \"The gift wrapping was beautiful and thoughtful.\",\n",
        "    \"The playground is clean, safe, and fun for children.\",\n",
        "    \"This chair is comfortable enough to sit in all day.\",\n",
        "    \"The webinar was informative and engaging.\",\n",
        "    \"My package arrived safely with everything intact.\",\n",
        "    \"The air conditioning works perfectly and cools the room fast.\",\n",
        "    \"The beach was peaceful and the water was crystal clear.\",\n",
        "    \"The app’s interface is simple yet powerful.\",\n",
        "    \"This bakery makes the best bread I have ever tasted.\",\n",
        "    \"The lecture was clear, engaging, and easy to follow.\",\n",
        "    \"I enjoyed exploring the park’s scenic walking trails.\",\n",
        "    \"The painting adds so much warmth to my living room.\",\n",
        "    \"The training sessions were motivating and well-paced.\",\n",
        "    \"Their service is reliable and always on time.\",\n",
        "    \"The coffee was rich and flavorful, just how I like it.\",\n",
        "    \"I feel energized after attending the morning yoga class.\",\n",
        "    \"The meal portions were generous and satisfying.\",\n",
        "    \"I admire the creativity behind this exhibition.\",\n",
        "    \"The interface updates made everything more intuitive.\",\n",
        "    \"The team handled the project with professionalism.\",\n",
        "    \"The scent of the candle is calming and long-lasting.\",\n",
        "    \"The bed is cozy and provided a restful sleep.\",\n",
        "    \"The online course was affordable and of high quality.\",\n",
        "    \"I appreciate the prompt responses to my emails.\",\n",
        "    \"The zoo had a wide variety of animals to see.\",\n",
        "    \"This watch is stylish and keeps accurate time.\",\n",
        "    \"The staff greeted me warmly at the entrance.\",\n",
        "    \"The flight was comfortable and landed on time.\",\n",
        "    \"I learned a lot from the seminar discussions.\",\n",
        "    \"The garden was colorful and well-maintained.\",\n",
        "    \"The tutorial videos were concise and clear.\",\n",
        "    \"The new update improved speed and performance.\",\n",
        "    \"I love how durable and sturdy this bag feels.\",\n",
        "    \"The food truck served the best tacos I’ve had.\",\n",
        "    \"The hiking trail was scenic and well-marked.\",\n",
        "    \"I enjoyed chatting with the friendly cashier.\",\n",
        "    \"The museum exhibits were fascinating and interactive.\",\n",
        "    \"The product is lightweight and easy to carry around.\",\n",
        "    \"The car handles smoothly on the road.\",\n",
        "    \"The concert hall had great acoustics.\",\n",
        "    \"The online platform is safe and easy to navigate.\",\n",
        "    \"The swimming pool was clean and inviting.\",\n",
        "    \"I liked the variety of topics covered in the workshop.\",\n",
        "    \"The service team exceeded my expectations.\",\n",
        "    \"The dessert was sweet, fresh, and perfectly baked.\",\n",
        "    \"The student community is active and helpful.\",\n",
        "    \"I was impressed by the professional presentation.\",\n",
        "    \"The coffee shop atmosphere is cozy and quiet.\",\n",
        "    \"The flight attendants were cheerful and attentive.\",\n",
        "    \"The training materials were practical and easy to apply.\",\n",
        "    \"The shoes are comfortable and stylish at the same time.\",\n",
        "    \"The picnic area was spacious and well-kept.\",\n",
        "    \"The instructions were easy to follow and accurate.\",\n",
        "    \"The gym has modern equipment and friendly staff.\",\n",
        "    \"The ice cream was smooth and creamy.\",\n",
        "    \"The teacher explained complex ideas very clearly.\",\n",
        "    \"The customer loyalty program offers great rewards.\",\n",
        "    \"The flowers arrived fresh and beautifully arranged.\",\n",
        "    \"The outdoor market was lively and full of variety.\",\n",
        "    \"The online registration process was quick and simple.\",\n",
        "    \"The art workshop sparked my creativity.\",\n",
        "    \"The theater had comfortable seating and clear sound.\",\n",
        "    \"I enjoyed the interactive games at the event.\",\n",
        "    \"The hiking guide was knowledgeable and supportive.\",\n",
        "    \"The package design is neat and attractive.\",\n",
        "    \"The volunteers made the charity event inspiring.\",\n",
        "    \"The photo quality exceeded what I expected.\",\n",
        "    \"The garden café had a lovely and calm atmosphere.\",\n",
        "    \"The new feature makes my work more efficient.\",\n",
        "    \"The sports event was exciting from start to finish.\",\n",
        "    \"The new policy made the workplace more inclusive.\",\n",
        "    \"The breakfast buffet had a wide variety of options.\",\n",
        "    \"The friendly neighbors made me feel at home.\",\n",
        "    \"The school trip was fun and educational.\",\n",
        "    \"I felt relaxed during the entire spa treatment.\",\n",
        "    \"The decorations created a cheerful environment.\",\n",
        "    \"The pet adoption process was smooth and positive.\"\n",
        "]\n",
        "\n",
        "negative_sentences = [\n",
        "    \"The update made everything slower and frustrating.\",\n",
        "    \"Customer support kept me waiting far too long.\",\n",
        "    \"The food was bland and lacked seasoning.\",\n",
        "    \"The delivery was late and items were missing.\",\n",
        "    \"The instructions were confusing and poorly written.\",\n",
        "    \"The movie was dull and dragged on for too long.\",\n",
        "    \"The product broke within a week of use.\",\n",
        "    \"The classroom was noisy and disorganized.\",\n",
        "    \"The service was impolite and unprofessional.\",\n",
        "    \"The hotel room smelled unpleasant and felt dirty.\",\n",
        "    \"This app keeps crashing and losing my data.\",\n",
        "    \"The event was poorly planned and chaotic.\",\n",
        "    \"The packaging was damaged when it arrived.\",\n",
        "    \"The flight was delayed without explanation.\",\n",
        "    \"The desk is unstable and wobbly.\",\n",
        "    \"The meal portions were small and unsatisfying.\",\n",
        "    \"The presentation lacked structure and clarity.\",\n",
        "    \"The website layout is confusing and outdated.\",\n",
        "    \"The ride was uncomfortable and bumpy.\",\n",
        "    \"The phone battery drains too quickly.\",\n",
        "    \"The chairs in the hall were hard and uncomfortable.\",\n",
        "    \"The service took longer than promised.\",\n",
        "    \"The café was noisy and overcrowded.\",\n",
        "    \"The online system kept logging me out.\",\n",
        "    \"The support team gave me unhelpful answers.\",\n",
        "    \"The shoes wore out after just a month.\",\n",
        "    \"The teacher spoke too fast and was hard to follow.\",\n",
        "    \"The air conditioning was too loud in the room.\",\n",
        "    \"The workshop content felt shallow and repetitive.\",\n",
        "    \"The new update removed features I liked.\",\n",
        "    \"The music at the restaurant was too loud.\",\n",
        "    \"The room lighting was dim and unpleasant.\",\n",
        "    \"The parking lot was small and disorganized.\",\n",
        "    \"The game has too many bugs and glitches.\",\n",
        "    \"The line at the counter moved very slowly.\",\n",
        "    \"The garden looked neglected and overgrown.\",\n",
        "    \"The concert started late and ended abruptly.\",\n",
        "    \"The app interface feels cluttered and outdated.\",\n",
        "    \"The store had very limited product choices.\",\n",
        "    \"The delivery driver was rude and dismissive.\",\n",
        "    \"The blanket felt rough and uncomfortable.\",\n",
        "    \"The seminar was boring and lacked interaction.\",\n",
        "    \"The dessert was dry and tasteless.\",\n",
        "    \"The meeting dragged on without direction.\",\n",
        "    \"The gym equipment was old and broken.\",\n",
        "    \"The instructions were missing important steps.\",\n",
        "    \"The support chat disconnected repeatedly.\",\n",
        "    \"The online course lacked real examples.\",\n",
        "    \"The toy broke the first time it was used.\",\n",
        "    \"The software installation kept failing.\",\n",
        "    \"The staff ignored my requests for help.\",\n",
        "    \"The water pressure in the shower was weak.\",\n",
        "    \"The new feature made navigation harder.\",\n",
        "    \"The road was full of potholes and unsafe.\",\n",
        "    \"The store shelves were dusty and disorganized.\",\n",
        "    \"The class felt rushed and incomplete.\",\n",
        "    \"The photo quality was blurry and disappointing.\",\n",
        "    \"The playground equipment was rusty and unsafe.\",\n",
        "    \"The bed was hard and uncomfortable to sleep on.\",\n",
        "    \"The coffee tasted bitter and burnt.\",\n",
        "    \"The new policy made things more complicated.\",\n",
        "    \"The bus was overcrowded and uncomfortable.\",\n",
        "    \"The email response came too late to be useful.\",\n",
        "    \"The waiter forgot part of my order.\",\n",
        "    \"The app uses too much phone storage.\",\n",
        "    \"The lecture slides were hard to read.\",\n",
        "    \"The shoes were stiff and gave me blisters.\",\n",
        "    \"The market was chaotic and overpriced.\",\n",
        "    \"The pool water looked cloudy and unclean.\",\n",
        "    \"The package arrived much later than promised.\",\n",
        "    \"The project lacked proper organization.\",\n",
        "    \"The movie ending felt rushed and unsatisfying.\",\n",
        "    \"The music sounded distorted through the speakers.\",\n",
        "    \"The staff seemed uninterested in helping.\",\n",
        "    \"The delivery box was crushed on arrival.\",\n",
        "    \"The class materials were outdated and dull.\",\n",
        "    \"The system crashed in the middle of my work.\",\n",
        "    \"The meal was too salty and greasy.\",\n",
        "    \"The light bulbs flickered constantly.\",\n",
        "    \"The garden café was swarming with insects.\",\n",
        "    \"The phone case cracked easily.\",\n",
        "    \"The sports match lacked excitement.\",\n",
        "    \"The directions were unclear and misleading.\",\n",
        "    \"The museum exhibits were sparse and uninteresting.\",\n",
        "    \"The bus ride was noisy and uncomfortable.\",\n",
        "    \"The training lacked depth and detail.\",\n",
        "    \"The table wobbled whenever I touched it.\",\n",
        "    \"The event started late and ran out of seats.\",\n",
        "    \"The drinks tasted watered down.\",\n",
        "    \"The theater chairs were cramped and worn out.\",\n",
        "    \"The online payment system kept failing.\",\n",
        "    \"The carpet in the room was stained.\",\n",
        "    \"The workshop facilitator seemed unprepared.\",\n",
        "    \"The headphones broke after minimal use.\",\n",
        "    \"The customer queue was disorganized.\",\n",
        "    \"The app notifications were annoying and constant.\",\n",
        "    \"The breakfast buffet had stale pastries.\",\n",
        "    \"The waiting area was cold and uncomfortable.\",\n",
        "    \"The parking fees were unreasonably high.\",\n",
        "    \"The camera focus was slow and unreliable.\",\n",
        "    \"The lesson plan felt repetitive and unhelpful.\"\n",
        "]"
      ],
      "metadata": {
        "id": "bLGSUWRsYdZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preproccess(text):\n",
        "  text = text.lower()\n",
        "  text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # punctuations\n",
        "  return text"
      ],
      "metadata": {
        "id": "UIA7zu9EYful"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "data = positive_sentences + negative_sentences\n",
        "labels = [1] * len(positive_sentences) + [0] * len(negative_sentences)"
      ],
      "metadata": {
        "id": "cLQjrNYb2H44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0PUhisl2SSM",
        "outputId": "e372ed88-fa56-4294-939d-b0fe19987fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [preproccess(sentence) for sentence in data ]\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXlIEvSd3Ion",
        "outputId": "5fa7f42c-63cd-4e0b-be10-5da4414e27d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this exceeded my expectations in every way',\n",
              " 'the staff was incredibly kind and helpful throughout my visit',\n",
              " 'i love how easy this app is to use on a daily basis',\n",
              " 'the course material was wellstructured and very informative',\n",
              " 'dinner was absolutely delicious with fresh ingredients and rich flavors',\n",
              " 'the movie had a touching story and brilliant performances',\n",
              " 'delivery arrived earlier than expected and everything was perfect',\n",
              " 'i enjoyed every moment of the concert the energy was amazing',\n",
              " 'the workplace atmosphere is supportive and inspiring',\n",
              " 'i appreciate how quickly customer support resolved my issue',\n",
              " 'the hotel room was spotless and had a beautiful view',\n",
              " 'this laptop runs smoothly and handles all my tasks efficiently',\n",
              " 'the tutorial made learning much easier than i anticipated',\n",
              " 'i felt very welcome at the event and met great people',\n",
              " 'the design is elegant and practical at the same time',\n",
              " 'traveling with this agency was stressfree and enjoyable',\n",
              " 'the workshop provided valuable skills i can use immediately',\n",
              " 'every staff member went out of their way to make me comfortable',\n",
              " 'the performance was breathtaking and unforgettable',\n",
              " 'i found the book to be insightful and beautifully written',\n",
              " 'this phone’s camera takes sharp and vibrant photos',\n",
              " 'the ride was smooth and the driver was very polite',\n",
              " 'i was impressed by the quick installation and setup process',\n",
              " 'the gift wrapping was beautiful and thoughtful',\n",
              " 'the playground is clean safe and fun for children',\n",
              " 'this chair is comfortable enough to sit in all day',\n",
              " 'the webinar was informative and engaging',\n",
              " 'my package arrived safely with everything intact',\n",
              " 'the air conditioning works perfectly and cools the room fast',\n",
              " 'the beach was peaceful and the water was crystal clear',\n",
              " 'the app’s interface is simple yet powerful',\n",
              " 'this bakery makes the best bread i have ever tasted',\n",
              " 'the lecture was clear engaging and easy to follow',\n",
              " 'i enjoyed exploring the park’s scenic walking trails',\n",
              " 'the painting adds so much warmth to my living room',\n",
              " 'the training sessions were motivating and wellpaced',\n",
              " 'their service is reliable and always on time',\n",
              " 'the coffee was rich and flavorful just how i like it',\n",
              " 'i feel energized after attending the morning yoga class',\n",
              " 'the meal portions were generous and satisfying',\n",
              " 'i admire the creativity behind this exhibition',\n",
              " 'the interface updates made everything more intuitive',\n",
              " 'the team handled the project with professionalism',\n",
              " 'the scent of the candle is calming and longlasting',\n",
              " 'the bed is cozy and provided a restful sleep',\n",
              " 'the online course was affordable and of high quality',\n",
              " 'i appreciate the prompt responses to my emails',\n",
              " 'the zoo had a wide variety of animals to see',\n",
              " 'this watch is stylish and keeps accurate time',\n",
              " 'the staff greeted me warmly at the entrance',\n",
              " 'the flight was comfortable and landed on time',\n",
              " 'i learned a lot from the seminar discussions',\n",
              " 'the garden was colorful and wellmaintained',\n",
              " 'the tutorial videos were concise and clear',\n",
              " 'the new update improved speed and performance',\n",
              " 'i love how durable and sturdy this bag feels',\n",
              " 'the food truck served the best tacos i’ve had',\n",
              " 'the hiking trail was scenic and wellmarked',\n",
              " 'i enjoyed chatting with the friendly cashier',\n",
              " 'the museum exhibits were fascinating and interactive',\n",
              " 'the product is lightweight and easy to carry around',\n",
              " 'the car handles smoothly on the road',\n",
              " 'the concert hall had great acoustics',\n",
              " 'the online platform is safe and easy to navigate',\n",
              " 'the swimming pool was clean and inviting',\n",
              " 'i liked the variety of topics covered in the workshop',\n",
              " 'the service team exceeded my expectations',\n",
              " 'the dessert was sweet fresh and perfectly baked',\n",
              " 'the student community is active and helpful',\n",
              " 'i was impressed by the professional presentation',\n",
              " 'the coffee shop atmosphere is cozy and quiet',\n",
              " 'the flight attendants were cheerful and attentive',\n",
              " 'the training materials were practical and easy to apply',\n",
              " 'the shoes are comfortable and stylish at the same time',\n",
              " 'the picnic area was spacious and wellkept',\n",
              " 'the instructions were easy to follow and accurate',\n",
              " 'the gym has modern equipment and friendly staff',\n",
              " 'the ice cream was smooth and creamy',\n",
              " 'the teacher explained complex ideas very clearly',\n",
              " 'the customer loyalty program offers great rewards',\n",
              " 'the flowers arrived fresh and beautifully arranged',\n",
              " 'the outdoor market was lively and full of variety',\n",
              " 'the online registration process was quick and simple',\n",
              " 'the art workshop sparked my creativity',\n",
              " 'the theater had comfortable seating and clear sound',\n",
              " 'i enjoyed the interactive games at the event',\n",
              " 'the hiking guide was knowledgeable and supportive',\n",
              " 'the package design is neat and attractive',\n",
              " 'the volunteers made the charity event inspiring',\n",
              " 'the photo quality exceeded what i expected',\n",
              " 'the garden café had a lovely and calm atmosphere',\n",
              " 'the new feature makes my work more efficient',\n",
              " 'the sports event was exciting from start to finish',\n",
              " 'the new policy made the workplace more inclusive',\n",
              " 'the breakfast buffet had a wide variety of options',\n",
              " 'the friendly neighbors made me feel at home',\n",
              " 'the school trip was fun and educational',\n",
              " 'i felt relaxed during the entire spa treatment',\n",
              " 'the decorations created a cheerful environment',\n",
              " 'the pet adoption process was smooth and positive',\n",
              " 'the update made everything slower and frustrating',\n",
              " 'customer support kept me waiting far too long',\n",
              " 'the food was bland and lacked seasoning',\n",
              " 'the delivery was late and items were missing',\n",
              " 'the instructions were confusing and poorly written',\n",
              " 'the movie was dull and dragged on for too long',\n",
              " 'the product broke within a week of use',\n",
              " 'the classroom was noisy and disorganized',\n",
              " 'the service was impolite and unprofessional',\n",
              " 'the hotel room smelled unpleasant and felt dirty',\n",
              " 'this app keeps crashing and losing my data',\n",
              " 'the event was poorly planned and chaotic',\n",
              " 'the packaging was damaged when it arrived',\n",
              " 'the flight was delayed without explanation',\n",
              " 'the desk is unstable and wobbly',\n",
              " 'the meal portions were small and unsatisfying',\n",
              " 'the presentation lacked structure and clarity',\n",
              " 'the website layout is confusing and outdated',\n",
              " 'the ride was uncomfortable and bumpy',\n",
              " 'the phone battery drains too quickly',\n",
              " 'the chairs in the hall were hard and uncomfortable',\n",
              " 'the service took longer than promised',\n",
              " 'the café was noisy and overcrowded',\n",
              " 'the online system kept logging me out',\n",
              " 'the support team gave me unhelpful answers',\n",
              " 'the shoes wore out after just a month',\n",
              " 'the teacher spoke too fast and was hard to follow',\n",
              " 'the air conditioning was too loud in the room',\n",
              " 'the workshop content felt shallow and repetitive',\n",
              " 'the new update removed features i liked',\n",
              " 'the music at the restaurant was too loud',\n",
              " 'the room lighting was dim and unpleasant',\n",
              " 'the parking lot was small and disorganized',\n",
              " 'the game has too many bugs and glitches',\n",
              " 'the line at the counter moved very slowly',\n",
              " 'the garden looked neglected and overgrown',\n",
              " 'the concert started late and ended abruptly',\n",
              " 'the app interface feels cluttered and outdated',\n",
              " 'the store had very limited product choices',\n",
              " 'the delivery driver was rude and dismissive',\n",
              " 'the blanket felt rough and uncomfortable',\n",
              " 'the seminar was boring and lacked interaction',\n",
              " 'the dessert was dry and tasteless',\n",
              " 'the meeting dragged on without direction',\n",
              " 'the gym equipment was old and broken',\n",
              " 'the instructions were missing important steps',\n",
              " 'the support chat disconnected repeatedly',\n",
              " 'the online course lacked real examples',\n",
              " 'the toy broke the first time it was used',\n",
              " 'the software installation kept failing',\n",
              " 'the staff ignored my requests for help',\n",
              " 'the water pressure in the shower was weak',\n",
              " 'the new feature made navigation harder',\n",
              " 'the road was full of potholes and unsafe',\n",
              " 'the store shelves were dusty and disorganized',\n",
              " 'the class felt rushed and incomplete',\n",
              " 'the photo quality was blurry and disappointing',\n",
              " 'the playground equipment was rusty and unsafe',\n",
              " 'the bed was hard and uncomfortable to sleep on',\n",
              " 'the coffee tasted bitter and burnt',\n",
              " 'the new policy made things more complicated',\n",
              " 'the bus was overcrowded and uncomfortable',\n",
              " 'the email response came too late to be useful',\n",
              " 'the waiter forgot part of my order',\n",
              " 'the app uses too much phone storage',\n",
              " 'the lecture slides were hard to read',\n",
              " 'the shoes were stiff and gave me blisters',\n",
              " 'the market was chaotic and overpriced',\n",
              " 'the pool water looked cloudy and unclean',\n",
              " 'the package arrived much later than promised',\n",
              " 'the project lacked proper organization',\n",
              " 'the movie ending felt rushed and unsatisfying',\n",
              " 'the music sounded distorted through the speakers',\n",
              " 'the staff seemed uninterested in helping',\n",
              " 'the delivery box was crushed on arrival',\n",
              " 'the class materials were outdated and dull',\n",
              " 'the system crashed in the middle of my work',\n",
              " 'the meal was too salty and greasy',\n",
              " 'the light bulbs flickered constantly',\n",
              " 'the garden café was swarming with insects',\n",
              " 'the phone case cracked easily',\n",
              " 'the sports match lacked excitement',\n",
              " 'the directions were unclear and misleading',\n",
              " 'the museum exhibits were sparse and uninteresting',\n",
              " 'the bus ride was noisy and uncomfortable',\n",
              " 'the training lacked depth and detail',\n",
              " 'the table wobbled whenever i touched it',\n",
              " 'the event started late and ran out of seats',\n",
              " 'the drinks tasted watered down',\n",
              " 'the theater chairs were cramped and worn out',\n",
              " 'the online payment system kept failing',\n",
              " 'the carpet in the room was stained',\n",
              " 'the workshop facilitator seemed unprepared',\n",
              " 'the headphones broke after minimal use',\n",
              " 'the customer queue was disorganized',\n",
              " 'the app notifications were annoying and constant',\n",
              " 'the breakfast buffet had stale pastries',\n",
              " 'the waiting area was cold and uncomfortable',\n",
              " 'the parking fees were unreasonably high',\n",
              " 'the camera focus was slow and unreliable',\n",
              " 'the lesson plan felt repetitive and unhelpful']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab\n",
        "all_word = \" \".join(data).split()\n",
        "word_counts = Counter(all_word)"
      ],
      "metadata": {
        "id": "0yVNT_UW3Sxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt2r20hm3jyM",
        "outputId": "d29efbda-b929-4568-ab42-9166d57eadc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "626"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {word: idx+1 for idx, (word, _) in enumerate(word_counts.items())}"
      ],
      "metadata": {
        "id": "PzrdMzWk3vCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-E81eoT4Vco",
        "outputId": "828a93cc-329c-4913-e9c3-48ed341bc3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 1,\n",
              " 'exceeded': 2,\n",
              " 'my': 3,\n",
              " 'expectations': 4,\n",
              " 'in': 5,\n",
              " 'every': 6,\n",
              " 'way': 7,\n",
              " 'the': 8,\n",
              " 'staff': 9,\n",
              " 'was': 10,\n",
              " 'incredibly': 11,\n",
              " 'kind': 12,\n",
              " 'and': 13,\n",
              " 'helpful': 14,\n",
              " 'throughout': 15,\n",
              " 'visit': 16,\n",
              " 'i': 17,\n",
              " 'love': 18,\n",
              " 'how': 19,\n",
              " 'easy': 20,\n",
              " 'app': 21,\n",
              " 'is': 22,\n",
              " 'to': 23,\n",
              " 'use': 24,\n",
              " 'on': 25,\n",
              " 'a': 26,\n",
              " 'daily': 27,\n",
              " 'basis': 28,\n",
              " 'course': 29,\n",
              " 'material': 30,\n",
              " 'wellstructured': 31,\n",
              " 'very': 32,\n",
              " 'informative': 33,\n",
              " 'dinner': 34,\n",
              " 'absolutely': 35,\n",
              " 'delicious': 36,\n",
              " 'with': 37,\n",
              " 'fresh': 38,\n",
              " 'ingredients': 39,\n",
              " 'rich': 40,\n",
              " 'flavors': 41,\n",
              " 'movie': 42,\n",
              " 'had': 43,\n",
              " 'touching': 44,\n",
              " 'story': 45,\n",
              " 'brilliant': 46,\n",
              " 'performances': 47,\n",
              " 'delivery': 48,\n",
              " 'arrived': 49,\n",
              " 'earlier': 50,\n",
              " 'than': 51,\n",
              " 'expected': 52,\n",
              " 'everything': 53,\n",
              " 'perfect': 54,\n",
              " 'enjoyed': 55,\n",
              " 'moment': 56,\n",
              " 'of': 57,\n",
              " 'concert': 58,\n",
              " 'energy': 59,\n",
              " 'amazing': 60,\n",
              " 'workplace': 61,\n",
              " 'atmosphere': 62,\n",
              " 'supportive': 63,\n",
              " 'inspiring': 64,\n",
              " 'appreciate': 65,\n",
              " 'quickly': 66,\n",
              " 'customer': 67,\n",
              " 'support': 68,\n",
              " 'resolved': 69,\n",
              " 'issue': 70,\n",
              " 'hotel': 71,\n",
              " 'room': 72,\n",
              " 'spotless': 73,\n",
              " 'beautiful': 74,\n",
              " 'view': 75,\n",
              " 'laptop': 76,\n",
              " 'runs': 77,\n",
              " 'smoothly': 78,\n",
              " 'handles': 79,\n",
              " 'all': 80,\n",
              " 'tasks': 81,\n",
              " 'efficiently': 82,\n",
              " 'tutorial': 83,\n",
              " 'made': 84,\n",
              " 'learning': 85,\n",
              " 'much': 86,\n",
              " 'easier': 87,\n",
              " 'anticipated': 88,\n",
              " 'felt': 89,\n",
              " 'welcome': 90,\n",
              " 'at': 91,\n",
              " 'event': 92,\n",
              " 'met': 93,\n",
              " 'great': 94,\n",
              " 'people': 95,\n",
              " 'design': 96,\n",
              " 'elegant': 97,\n",
              " 'practical': 98,\n",
              " 'same': 99,\n",
              " 'time': 100,\n",
              " 'traveling': 101,\n",
              " 'agency': 102,\n",
              " 'stressfree': 103,\n",
              " 'enjoyable': 104,\n",
              " 'workshop': 105,\n",
              " 'provided': 106,\n",
              " 'valuable': 107,\n",
              " 'skills': 108,\n",
              " 'can': 109,\n",
              " 'immediately': 110,\n",
              " 'member': 111,\n",
              " 'went': 112,\n",
              " 'out': 113,\n",
              " 'their': 114,\n",
              " 'make': 115,\n",
              " 'me': 116,\n",
              " 'comfortable': 117,\n",
              " 'performance': 118,\n",
              " 'breathtaking': 119,\n",
              " 'unforgettable': 120,\n",
              " 'found': 121,\n",
              " 'book': 122,\n",
              " 'be': 123,\n",
              " 'insightful': 124,\n",
              " 'beautifully': 125,\n",
              " 'written': 126,\n",
              " 'phone’s': 127,\n",
              " 'camera': 128,\n",
              " 'takes': 129,\n",
              " 'sharp': 130,\n",
              " 'vibrant': 131,\n",
              " 'photos': 132,\n",
              " 'ride': 133,\n",
              " 'smooth': 134,\n",
              " 'driver': 135,\n",
              " 'polite': 136,\n",
              " 'impressed': 137,\n",
              " 'by': 138,\n",
              " 'quick': 139,\n",
              " 'installation': 140,\n",
              " 'setup': 141,\n",
              " 'process': 142,\n",
              " 'gift': 143,\n",
              " 'wrapping': 144,\n",
              " 'thoughtful': 145,\n",
              " 'playground': 146,\n",
              " 'clean': 147,\n",
              " 'safe': 148,\n",
              " 'fun': 149,\n",
              " 'for': 150,\n",
              " 'children': 151,\n",
              " 'chair': 152,\n",
              " 'enough': 153,\n",
              " 'sit': 154,\n",
              " 'day': 155,\n",
              " 'webinar': 156,\n",
              " 'engaging': 157,\n",
              " 'package': 158,\n",
              " 'safely': 159,\n",
              " 'intact': 160,\n",
              " 'air': 161,\n",
              " 'conditioning': 162,\n",
              " 'works': 163,\n",
              " 'perfectly': 164,\n",
              " 'cools': 165,\n",
              " 'fast': 166,\n",
              " 'beach': 167,\n",
              " 'peaceful': 168,\n",
              " 'water': 169,\n",
              " 'crystal': 170,\n",
              " 'clear': 171,\n",
              " 'app’s': 172,\n",
              " 'interface': 173,\n",
              " 'simple': 174,\n",
              " 'yet': 175,\n",
              " 'powerful': 176,\n",
              " 'bakery': 177,\n",
              " 'makes': 178,\n",
              " 'best': 179,\n",
              " 'bread': 180,\n",
              " 'have': 181,\n",
              " 'ever': 182,\n",
              " 'tasted': 183,\n",
              " 'lecture': 184,\n",
              " 'follow': 185,\n",
              " 'exploring': 186,\n",
              " 'park’s': 187,\n",
              " 'scenic': 188,\n",
              " 'walking': 189,\n",
              " 'trails': 190,\n",
              " 'painting': 191,\n",
              " 'adds': 192,\n",
              " 'so': 193,\n",
              " 'warmth': 194,\n",
              " 'living': 195,\n",
              " 'training': 196,\n",
              " 'sessions': 197,\n",
              " 'were': 198,\n",
              " 'motivating': 199,\n",
              " 'wellpaced': 200,\n",
              " 'service': 201,\n",
              " 'reliable': 202,\n",
              " 'always': 203,\n",
              " 'coffee': 204,\n",
              " 'flavorful': 205,\n",
              " 'just': 206,\n",
              " 'like': 207,\n",
              " 'it': 208,\n",
              " 'feel': 209,\n",
              " 'energized': 210,\n",
              " 'after': 211,\n",
              " 'attending': 212,\n",
              " 'morning': 213,\n",
              " 'yoga': 214,\n",
              " 'class': 215,\n",
              " 'meal': 216,\n",
              " 'portions': 217,\n",
              " 'generous': 218,\n",
              " 'satisfying': 219,\n",
              " 'admire': 220,\n",
              " 'creativity': 221,\n",
              " 'behind': 222,\n",
              " 'exhibition': 223,\n",
              " 'updates': 224,\n",
              " 'more': 225,\n",
              " 'intuitive': 226,\n",
              " 'team': 227,\n",
              " 'handled': 228,\n",
              " 'project': 229,\n",
              " 'professionalism': 230,\n",
              " 'scent': 231,\n",
              " 'candle': 232,\n",
              " 'calming': 233,\n",
              " 'longlasting': 234,\n",
              " 'bed': 235,\n",
              " 'cozy': 236,\n",
              " 'restful': 237,\n",
              " 'sleep': 238,\n",
              " 'online': 239,\n",
              " 'affordable': 240,\n",
              " 'high': 241,\n",
              " 'quality': 242,\n",
              " 'prompt': 243,\n",
              " 'responses': 244,\n",
              " 'emails': 245,\n",
              " 'zoo': 246,\n",
              " 'wide': 247,\n",
              " 'variety': 248,\n",
              " 'animals': 249,\n",
              " 'see': 250,\n",
              " 'watch': 251,\n",
              " 'stylish': 252,\n",
              " 'keeps': 253,\n",
              " 'accurate': 254,\n",
              " 'greeted': 255,\n",
              " 'warmly': 256,\n",
              " 'entrance': 257,\n",
              " 'flight': 258,\n",
              " 'landed': 259,\n",
              " 'learned': 260,\n",
              " 'lot': 261,\n",
              " 'from': 262,\n",
              " 'seminar': 263,\n",
              " 'discussions': 264,\n",
              " 'garden': 265,\n",
              " 'colorful': 266,\n",
              " 'wellmaintained': 267,\n",
              " 'videos': 268,\n",
              " 'concise': 269,\n",
              " 'new': 270,\n",
              " 'update': 271,\n",
              " 'improved': 272,\n",
              " 'speed': 273,\n",
              " 'durable': 274,\n",
              " 'sturdy': 275,\n",
              " 'bag': 276,\n",
              " 'feels': 277,\n",
              " 'food': 278,\n",
              " 'truck': 279,\n",
              " 'served': 280,\n",
              " 'tacos': 281,\n",
              " 'i’ve': 282,\n",
              " 'hiking': 283,\n",
              " 'trail': 284,\n",
              " 'wellmarked': 285,\n",
              " 'chatting': 286,\n",
              " 'friendly': 287,\n",
              " 'cashier': 288,\n",
              " 'museum': 289,\n",
              " 'exhibits': 290,\n",
              " 'fascinating': 291,\n",
              " 'interactive': 292,\n",
              " 'product': 293,\n",
              " 'lightweight': 294,\n",
              " 'carry': 295,\n",
              " 'around': 296,\n",
              " 'car': 297,\n",
              " 'road': 298,\n",
              " 'hall': 299,\n",
              " 'acoustics': 300,\n",
              " 'platform': 301,\n",
              " 'navigate': 302,\n",
              " 'swimming': 303,\n",
              " 'pool': 304,\n",
              " 'inviting': 305,\n",
              " 'liked': 306,\n",
              " 'topics': 307,\n",
              " 'covered': 308,\n",
              " 'dessert': 309,\n",
              " 'sweet': 310,\n",
              " 'baked': 311,\n",
              " 'student': 312,\n",
              " 'community': 313,\n",
              " 'active': 314,\n",
              " 'professional': 315,\n",
              " 'presentation': 316,\n",
              " 'shop': 317,\n",
              " 'quiet': 318,\n",
              " 'attendants': 319,\n",
              " 'cheerful': 320,\n",
              " 'attentive': 321,\n",
              " 'materials': 322,\n",
              " 'apply': 323,\n",
              " 'shoes': 324,\n",
              " 'are': 325,\n",
              " 'picnic': 326,\n",
              " 'area': 327,\n",
              " 'spacious': 328,\n",
              " 'wellkept': 329,\n",
              " 'instructions': 330,\n",
              " 'gym': 331,\n",
              " 'has': 332,\n",
              " 'modern': 333,\n",
              " 'equipment': 334,\n",
              " 'ice': 335,\n",
              " 'cream': 336,\n",
              " 'creamy': 337,\n",
              " 'teacher': 338,\n",
              " 'explained': 339,\n",
              " 'complex': 340,\n",
              " 'ideas': 341,\n",
              " 'clearly': 342,\n",
              " 'loyalty': 343,\n",
              " 'program': 344,\n",
              " 'offers': 345,\n",
              " 'rewards': 346,\n",
              " 'flowers': 347,\n",
              " 'arranged': 348,\n",
              " 'outdoor': 349,\n",
              " 'market': 350,\n",
              " 'lively': 351,\n",
              " 'full': 352,\n",
              " 'registration': 353,\n",
              " 'art': 354,\n",
              " 'sparked': 355,\n",
              " 'theater': 356,\n",
              " 'seating': 357,\n",
              " 'sound': 358,\n",
              " 'games': 359,\n",
              " 'guide': 360,\n",
              " 'knowledgeable': 361,\n",
              " 'neat': 362,\n",
              " 'attractive': 363,\n",
              " 'volunteers': 364,\n",
              " 'charity': 365,\n",
              " 'photo': 366,\n",
              " 'what': 367,\n",
              " 'café': 368,\n",
              " 'lovely': 369,\n",
              " 'calm': 370,\n",
              " 'feature': 371,\n",
              " 'work': 372,\n",
              " 'efficient': 373,\n",
              " 'sports': 374,\n",
              " 'exciting': 375,\n",
              " 'start': 376,\n",
              " 'finish': 377,\n",
              " 'policy': 378,\n",
              " 'inclusive': 379,\n",
              " 'breakfast': 380,\n",
              " 'buffet': 381,\n",
              " 'options': 382,\n",
              " 'neighbors': 383,\n",
              " 'home': 384,\n",
              " 'school': 385,\n",
              " 'trip': 386,\n",
              " 'educational': 387,\n",
              " 'relaxed': 388,\n",
              " 'during': 389,\n",
              " 'entire': 390,\n",
              " 'spa': 391,\n",
              " 'treatment': 392,\n",
              " 'decorations': 393,\n",
              " 'created': 394,\n",
              " 'environment': 395,\n",
              " 'pet': 396,\n",
              " 'adoption': 397,\n",
              " 'positive': 398,\n",
              " 'slower': 399,\n",
              " 'frustrating': 400,\n",
              " 'kept': 401,\n",
              " 'waiting': 402,\n",
              " 'far': 403,\n",
              " 'too': 404,\n",
              " 'long': 405,\n",
              " 'bland': 406,\n",
              " 'lacked': 407,\n",
              " 'seasoning': 408,\n",
              " 'late': 409,\n",
              " 'items': 410,\n",
              " 'missing': 411,\n",
              " 'confusing': 412,\n",
              " 'poorly': 413,\n",
              " 'dull': 414,\n",
              " 'dragged': 415,\n",
              " 'broke': 416,\n",
              " 'within': 417,\n",
              " 'week': 418,\n",
              " 'classroom': 419,\n",
              " 'noisy': 420,\n",
              " 'disorganized': 421,\n",
              " 'impolite': 422,\n",
              " 'unprofessional': 423,\n",
              " 'smelled': 424,\n",
              " 'unpleasant': 425,\n",
              " 'dirty': 426,\n",
              " 'crashing': 427,\n",
              " 'losing': 428,\n",
              " 'data': 429,\n",
              " 'planned': 430,\n",
              " 'chaotic': 431,\n",
              " 'packaging': 432,\n",
              " 'damaged': 433,\n",
              " 'when': 434,\n",
              " 'delayed': 435,\n",
              " 'without': 436,\n",
              " 'explanation': 437,\n",
              " 'desk': 438,\n",
              " 'unstable': 439,\n",
              " 'wobbly': 440,\n",
              " 'small': 441,\n",
              " 'unsatisfying': 442,\n",
              " 'structure': 443,\n",
              " 'clarity': 444,\n",
              " 'website': 445,\n",
              " 'layout': 446,\n",
              " 'outdated': 447,\n",
              " 'uncomfortable': 448,\n",
              " 'bumpy': 449,\n",
              " 'phone': 450,\n",
              " 'battery': 451,\n",
              " 'drains': 452,\n",
              " 'chairs': 453,\n",
              " 'hard': 454,\n",
              " 'took': 455,\n",
              " 'longer': 456,\n",
              " 'promised': 457,\n",
              " 'overcrowded': 458,\n",
              " 'system': 459,\n",
              " 'logging': 460,\n",
              " 'gave': 461,\n",
              " 'unhelpful': 462,\n",
              " 'answers': 463,\n",
              " 'wore': 464,\n",
              " 'month': 465,\n",
              " 'spoke': 466,\n",
              " 'loud': 467,\n",
              " 'content': 468,\n",
              " 'shallow': 469,\n",
              " 'repetitive': 470,\n",
              " 'removed': 471,\n",
              " 'features': 472,\n",
              " 'music': 473,\n",
              " 'restaurant': 474,\n",
              " 'lighting': 475,\n",
              " 'dim': 476,\n",
              " 'parking': 477,\n",
              " 'game': 478,\n",
              " 'many': 479,\n",
              " 'bugs': 480,\n",
              " 'glitches': 481,\n",
              " 'line': 482,\n",
              " 'counter': 483,\n",
              " 'moved': 484,\n",
              " 'slowly': 485,\n",
              " 'looked': 486,\n",
              " 'neglected': 487,\n",
              " 'overgrown': 488,\n",
              " 'started': 489,\n",
              " 'ended': 490,\n",
              " 'abruptly': 491,\n",
              " 'cluttered': 492,\n",
              " 'store': 493,\n",
              " 'limited': 494,\n",
              " 'choices': 495,\n",
              " 'rude': 496,\n",
              " 'dismissive': 497,\n",
              " 'blanket': 498,\n",
              " 'rough': 499,\n",
              " 'boring': 500,\n",
              " 'interaction': 501,\n",
              " 'dry': 502,\n",
              " 'tasteless': 503,\n",
              " 'meeting': 504,\n",
              " 'direction': 505,\n",
              " 'old': 506,\n",
              " 'broken': 507,\n",
              " 'important': 508,\n",
              " 'steps': 509,\n",
              " 'chat': 510,\n",
              " 'disconnected': 511,\n",
              " 'repeatedly': 512,\n",
              " 'real': 513,\n",
              " 'examples': 514,\n",
              " 'toy': 515,\n",
              " 'first': 516,\n",
              " 'used': 517,\n",
              " 'software': 518,\n",
              " 'failing': 519,\n",
              " 'ignored': 520,\n",
              " 'requests': 521,\n",
              " 'help': 522,\n",
              " 'pressure': 523,\n",
              " 'shower': 524,\n",
              " 'weak': 525,\n",
              " 'navigation': 526,\n",
              " 'harder': 527,\n",
              " 'potholes': 528,\n",
              " 'unsafe': 529,\n",
              " 'shelves': 530,\n",
              " 'dusty': 531,\n",
              " 'rushed': 532,\n",
              " 'incomplete': 533,\n",
              " 'blurry': 534,\n",
              " 'disappointing': 535,\n",
              " 'rusty': 536,\n",
              " 'bitter': 537,\n",
              " 'burnt': 538,\n",
              " 'things': 539,\n",
              " 'complicated': 540,\n",
              " 'bus': 541,\n",
              " 'email': 542,\n",
              " 'response': 543,\n",
              " 'came': 544,\n",
              " 'useful': 545,\n",
              " 'waiter': 546,\n",
              " 'forgot': 547,\n",
              " 'part': 548,\n",
              " 'order': 549,\n",
              " 'uses': 550,\n",
              " 'storage': 551,\n",
              " 'slides': 552,\n",
              " 'read': 553,\n",
              " 'stiff': 554,\n",
              " 'blisters': 555,\n",
              " 'overpriced': 556,\n",
              " 'cloudy': 557,\n",
              " 'unclean': 558,\n",
              " 'later': 559,\n",
              " 'proper': 560,\n",
              " 'organization': 561,\n",
              " 'ending': 562,\n",
              " 'sounded': 563,\n",
              " 'distorted': 564,\n",
              " 'through': 565,\n",
              " 'speakers': 566,\n",
              " 'seemed': 567,\n",
              " 'uninterested': 568,\n",
              " 'helping': 569,\n",
              " 'box': 570,\n",
              " 'crushed': 571,\n",
              " 'arrival': 572,\n",
              " 'crashed': 573,\n",
              " 'middle': 574,\n",
              " 'salty': 575,\n",
              " 'greasy': 576,\n",
              " 'light': 577,\n",
              " 'bulbs': 578,\n",
              " 'flickered': 579,\n",
              " 'constantly': 580,\n",
              " 'swarming': 581,\n",
              " 'insects': 582,\n",
              " 'case': 583,\n",
              " 'cracked': 584,\n",
              " 'easily': 585,\n",
              " 'match': 586,\n",
              " 'excitement': 587,\n",
              " 'directions': 588,\n",
              " 'unclear': 589,\n",
              " 'misleading': 590,\n",
              " 'sparse': 591,\n",
              " 'uninteresting': 592,\n",
              " 'depth': 593,\n",
              " 'detail': 594,\n",
              " 'table': 595,\n",
              " 'wobbled': 596,\n",
              " 'whenever': 597,\n",
              " 'touched': 598,\n",
              " 'ran': 599,\n",
              " 'seats': 600,\n",
              " 'drinks': 601,\n",
              " 'watered': 602,\n",
              " 'down': 603,\n",
              " 'cramped': 604,\n",
              " 'worn': 605,\n",
              " 'payment': 606,\n",
              " 'carpet': 607,\n",
              " 'stained': 608,\n",
              " 'facilitator': 609,\n",
              " 'unprepared': 610,\n",
              " 'headphones': 611,\n",
              " 'minimal': 612,\n",
              " 'queue': 613,\n",
              " 'notifications': 614,\n",
              " 'annoying': 615,\n",
              " 'constant': 616,\n",
              " 'stale': 617,\n",
              " 'pastries': 618,\n",
              " 'cold': 619,\n",
              " 'fees': 620,\n",
              " 'unreasonably': 621,\n",
              " 'focus': 622,\n",
              " 'slow': 623,\n",
              " 'unreliable': 624,\n",
              " 'lesson': 625,\n",
              " 'plan': 626}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[\"<PAD>\"] = 0 # NLP*** padding special token"
      ],
      "metadata": {
        "id": "KbqTQFrA4Y2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data to tensor\n",
        "max_len = 15\n",
        "def sentence_to_tensor(sentence, vocab, max_len = 15):\n",
        "  tokens = sentence.split()\n",
        "  indices = [vocab.get(word, 0) for word in tokens] # get index\n",
        "  indices = indices[:max_len]\n",
        "  indices += [0] * (max_len - len(indices))\n",
        "  return torch.tensor(indices)"
      ],
      "metadata": {
        "id": "2jM4Gkdh4xXI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.stack([sentence_to_tensor(sentence, vocab, max_len) for sentence in data])\n",
        "y = torch.tensor(labels)"
      ],
      "metadata": {
        "id": "OXJgk2SL7f9E"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CBG1myFyADol"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Transformers Model"
      ],
      "metadata": {
        "id": "X6K90LklSnkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClass(nn.Module): # inherit by nn\n",
        "  def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, num_classes):\n",
        "    super(TransformerClass, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.positional_encoding = nn.Parameter(torch.randn(1, max_len, embedding_dim))\n",
        "    self.transformer = nn.Transformer(d_model=embedding_dim, # dimension of embedding vector\n",
        "                                      nhead=num_heads, # number of head in multi-head attention\n",
        "                                      num_encoder_layers=num_layers, # number of transformer encoder\n",
        "                                      dim_feedforward=hidden_dim # dimension of hidden layer in feed-forward\n",
        "                                      )\n",
        "    self.fc = nn.Linear(embedding_dim*max_len, hidden_dim)\n",
        "    self.out = nn.Linear(hidden_dim, num_classes)\n",
        "    self.sigmoid = nn.Sigmoid() # binary classification\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x) + self.positional_encoding\n",
        "    output = self.transformer(embedded, embedded)\n",
        "    output = output.view(output.size(0), -1) # to resize\n",
        "    output = torch.relu(self.fc(output))\n",
        "    output = self.out(output)\n",
        "    output = self.sigmoid(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "UY4NNmFISrQe"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerClass(len(vocab),\n",
        "                         32, # embedding_dim\n",
        "                         4, # num_heads\n",
        "                         4, # num_layers\n",
        "                         64, # hidden_dim\n",
        "                         1 # num_classes\n",
        "                         )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lJzXBPEEFHg",
        "outputId": "2d4391ac-2a36-412a-b331-383f7be61429"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZj-O43iGcgL",
        "outputId": "d7fa8a1b-dad1-404a-a478-4cc8eb4bbe5b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerClass(\n",
              "  (embedding): Embedding(627, 32)\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-3): 4 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
              "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=32, out_features=64, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
              "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=480, out_features=64, bias=True)\n",
              "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "I5NeU1HWSr-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# determine model parameter - configs\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 32\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "hidden_dim = 64\n",
        "num_classes = 1 # actually, we have 2 classes but we applied sigmoid function. So, num_classes = 1. we must look outcome\n",
        "###determined hyperparameters"
      ],
      "metadata": {
        "id": "BPqGLT7qStVN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerClass(vocab_size, embedding_dim, num_heads, num_layers, hidden_dim, num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQmySb-RMR02",
        "outputId": "810f2f19-fe07-4ca6-ad77-78768346c8a1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "id": "Y8kx6coIMh-K"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "model.train()\n",
        "number_epochs = 10\n",
        "for epoch in range(number_epochs):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(X_train.long()).squeeze()\n",
        "  loss = criterion(output, y_train.float())\n",
        "  loss.backward() # calculate gradient\n",
        "  optimizer.step() # update parameters\n",
        "  print(f\"Epoch {epoch+1}/{number_epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mUrNjc1MutS",
        "outputId": "39073814-0357-4256-f90c-7085b9143f12"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.692378580570221\n",
            "Epoch 2/10, Loss: 0.6908241510391235\n",
            "Epoch 3/10, Loss: 0.6918665170669556\n",
            "Epoch 4/10, Loss: 0.6930682063102722\n",
            "Epoch 5/10, Loss: 0.6920795440673828\n",
            "Epoch 6/10, Loss: 0.6904513835906982\n",
            "Epoch 7/10, Loss: 0.6939241886138916\n",
            "Epoch 8/10, Loss: 0.6986981630325317\n",
            "Epoch 9/10, Loss: 0.6863440275192261\n",
            "Epoch 10/10, Loss: 0.6908623576164246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "number_epochs = 100\n",
        "for epoch in range(number_epochs):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(X_train.long()).squeeze()\n",
        "  loss = criterion(output, y_train.float())\n",
        "  loss.backward() # calculate gradient\n",
        "  optimizer.step() # update parameters\n",
        "  print(f\"Epoch {epoch+1}/{number_epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yejscdawNk8b",
        "outputId": "f460f273-e7bd-46d7-ebfe-77f271504145"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.6837903261184692\n",
            "Epoch 2/100, Loss: 0.6854389905929565\n",
            "Epoch 3/100, Loss: 0.6866539716720581\n",
            "Epoch 4/100, Loss: 0.682386577129364\n",
            "Epoch 5/100, Loss: 0.6867493987083435\n",
            "Epoch 6/100, Loss: 0.685524582862854\n",
            "Epoch 7/100, Loss: 0.6847081780433655\n",
            "Epoch 8/100, Loss: 0.6813600659370422\n",
            "Epoch 9/100, Loss: 0.6764861345291138\n",
            "Epoch 10/100, Loss: 0.6815594434738159\n",
            "Epoch 11/100, Loss: 0.6735302209854126\n",
            "Epoch 12/100, Loss: 0.6709972620010376\n",
            "Epoch 13/100, Loss: 0.6718109250068665\n",
            "Epoch 14/100, Loss: 0.6652923822402954\n",
            "Epoch 15/100, Loss: 0.6569326519966125\n",
            "Epoch 16/100, Loss: 0.6450566053390503\n",
            "Epoch 17/100, Loss: 0.6589081883430481\n",
            "Epoch 18/100, Loss: 0.6312363743782043\n",
            "Epoch 19/100, Loss: 0.6196647882461548\n",
            "Epoch 20/100, Loss: 0.6230730414390564\n",
            "Epoch 21/100, Loss: 0.6327977180480957\n",
            "Epoch 22/100, Loss: 0.6039118766784668\n",
            "Epoch 23/100, Loss: 0.5940648913383484\n",
            "Epoch 24/100, Loss: 0.5764705538749695\n",
            "Epoch 25/100, Loss: 0.5785576701164246\n",
            "Epoch 26/100, Loss: 0.5501976013183594\n",
            "Epoch 27/100, Loss: 0.5430349111557007\n",
            "Epoch 28/100, Loss: 0.555888831615448\n",
            "Epoch 29/100, Loss: 0.5148488283157349\n",
            "Epoch 30/100, Loss: 0.5341238975524902\n",
            "Epoch 31/100, Loss: 0.5188324451446533\n",
            "Epoch 32/100, Loss: 0.5207208395004272\n",
            "Epoch 33/100, Loss: 0.49794918298721313\n",
            "Epoch 34/100, Loss: 0.5057775974273682\n",
            "Epoch 35/100, Loss: 0.47296151518821716\n",
            "Epoch 36/100, Loss: 0.4867189824581146\n",
            "Epoch 37/100, Loss: 0.4610530734062195\n",
            "Epoch 38/100, Loss: 0.47040748596191406\n",
            "Epoch 39/100, Loss: 0.4735340476036072\n",
            "Epoch 40/100, Loss: 0.46598148345947266\n",
            "Epoch 41/100, Loss: 0.45610475540161133\n",
            "Epoch 42/100, Loss: 0.43051061034202576\n",
            "Epoch 43/100, Loss: 0.4566316604614258\n",
            "Epoch 44/100, Loss: 0.37713566422462463\n",
            "Epoch 45/100, Loss: 0.4239518642425537\n",
            "Epoch 46/100, Loss: 0.3823734521865845\n",
            "Epoch 47/100, Loss: 0.3777579665184021\n",
            "Epoch 48/100, Loss: 0.3722720742225647\n",
            "Epoch 49/100, Loss: 0.3832583725452423\n",
            "Epoch 50/100, Loss: 0.3844148516654968\n",
            "Epoch 51/100, Loss: 0.36954739689826965\n",
            "Epoch 52/100, Loss: 0.32596713304519653\n",
            "Epoch 53/100, Loss: 0.38647332787513733\n",
            "Epoch 54/100, Loss: 0.3302401900291443\n",
            "Epoch 55/100, Loss: 0.36262330412864685\n",
            "Epoch 56/100, Loss: 0.3183968663215637\n",
            "Epoch 57/100, Loss: 0.30108708143234253\n",
            "Epoch 58/100, Loss: 0.33939099311828613\n",
            "Epoch 59/100, Loss: 0.279246985912323\n",
            "Epoch 60/100, Loss: 0.2754593789577484\n",
            "Epoch 61/100, Loss: 0.3425893485546112\n",
            "Epoch 62/100, Loss: 0.31248044967651367\n",
            "Epoch 63/100, Loss: 0.29266002774238586\n",
            "Epoch 64/100, Loss: 0.2595883905887604\n",
            "Epoch 65/100, Loss: 0.29297736287117004\n",
            "Epoch 66/100, Loss: 0.2791716158390045\n",
            "Epoch 67/100, Loss: 0.2778030037879944\n",
            "Epoch 68/100, Loss: 0.27115797996520996\n",
            "Epoch 69/100, Loss: 0.26637160778045654\n",
            "Epoch 70/100, Loss: 0.25395822525024414\n",
            "Epoch 71/100, Loss: 0.2547982335090637\n",
            "Epoch 72/100, Loss: 0.23107895255088806\n",
            "Epoch 73/100, Loss: 0.22997334599494934\n",
            "Epoch 74/100, Loss: 0.2133551388978958\n",
            "Epoch 75/100, Loss: 0.23536527156829834\n",
            "Epoch 76/100, Loss: 0.20810453593730927\n",
            "Epoch 77/100, Loss: 0.2220720797777176\n",
            "Epoch 78/100, Loss: 0.24055716395378113\n",
            "Epoch 79/100, Loss: 0.19983358681201935\n",
            "Epoch 80/100, Loss: 0.22678890824317932\n",
            "Epoch 81/100, Loss: 0.20029902458190918\n",
            "Epoch 82/100, Loss: 0.20749644935131073\n",
            "Epoch 83/100, Loss: 0.16842308640480042\n",
            "Epoch 84/100, Loss: 0.1676977276802063\n",
            "Epoch 85/100, Loss: 0.1184733510017395\n",
            "Epoch 86/100, Loss: 0.1472519338130951\n",
            "Epoch 87/100, Loss: 0.13834981620311737\n",
            "Epoch 88/100, Loss: 0.13301341235637665\n",
            "Epoch 89/100, Loss: 0.14755749702453613\n",
            "Epoch 90/100, Loss: 0.13306739926338196\n",
            "Epoch 91/100, Loss: 0.13397298753261566\n",
            "Epoch 92/100, Loss: 0.11419578641653061\n",
            "Epoch 93/100, Loss: 0.15148845314979553\n",
            "Epoch 94/100, Loss: 0.12412357330322266\n",
            "Epoch 95/100, Loss: 0.12301446497440338\n",
            "Epoch 96/100, Loss: 0.11224806308746338\n",
            "Epoch 97/100, Loss: 0.11466063559055328\n",
            "Epoch 98/100, Loss: 0.11505293846130371\n",
            "Epoch 99/100, Loss: 0.10279306024312973\n",
            "Epoch 100/100, Loss: 0.09589363634586334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "3WStxQ8_St5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test.long()).squeeze()\n",
        "  y_pred = (y_pred > 0.5).float()\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "URjd4teiSvr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6670b5da-fd23-436d-b54a-8bdc8ca817b6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.43902439024390244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "number_epochs = 200\n",
        "for epoch in range(number_epochs):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(X_train.long()).squeeze()\n",
        "  loss = criterion(output, y_train.float())\n",
        "  loss.backward() # calculate gradient\n",
        "  optimizer.step() # update parameters\n",
        "  print(f\"Epoch {epoch+1}/{number_epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNcgiVUqOhLs",
        "outputId": "48675443-4982-4ee6-952d-00c5b71982f5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 0.1263490617275238\n",
            "Epoch 2/200, Loss: 0.06602473556995392\n",
            "Epoch 3/200, Loss: 0.08448107540607452\n",
            "Epoch 4/200, Loss: 0.11410275846719742\n",
            "Epoch 5/200, Loss: 0.11455168575048447\n",
            "Epoch 6/200, Loss: 0.0786178931593895\n",
            "Epoch 7/200, Loss: 0.07072366774082184\n",
            "Epoch 8/200, Loss: 0.10626480728387833\n",
            "Epoch 9/200, Loss: 0.058963410556316376\n",
            "Epoch 10/200, Loss: 0.0546434111893177\n",
            "Epoch 11/200, Loss: 0.09521989524364471\n",
            "Epoch 12/200, Loss: 0.07687585800886154\n",
            "Epoch 13/200, Loss: 0.08144913613796234\n",
            "Epoch 14/200, Loss: 0.07514689862728119\n",
            "Epoch 15/200, Loss: 0.056427985429763794\n",
            "Epoch 16/200, Loss: 0.07164297997951508\n",
            "Epoch 17/200, Loss: 0.06891940534114838\n",
            "Epoch 18/200, Loss: 0.058625005185604095\n",
            "Epoch 19/200, Loss: 0.04477633535861969\n",
            "Epoch 20/200, Loss: 0.03421150520443916\n",
            "Epoch 21/200, Loss: 0.04892243072390556\n",
            "Epoch 22/200, Loss: 0.04842100292444229\n",
            "Epoch 23/200, Loss: 0.07080323994159698\n",
            "Epoch 24/200, Loss: 0.045660749077796936\n",
            "Epoch 25/200, Loss: 0.03888706490397453\n",
            "Epoch 26/200, Loss: 0.0648733451962471\n",
            "Epoch 27/200, Loss: 0.039657048881053925\n",
            "Epoch 28/200, Loss: 0.07940836250782013\n",
            "Epoch 29/200, Loss: 0.037637192755937576\n",
            "Epoch 30/200, Loss: 0.032965969294309616\n",
            "Epoch 31/200, Loss: 0.04999438673257828\n",
            "Epoch 32/200, Loss: 0.030679970979690552\n",
            "Epoch 33/200, Loss: 0.029178351163864136\n",
            "Epoch 34/200, Loss: 0.04247116297483444\n",
            "Epoch 35/200, Loss: 0.036701176315546036\n",
            "Epoch 36/200, Loss: 0.032474584877491\n",
            "Epoch 37/200, Loss: 0.03467601165175438\n",
            "Epoch 38/200, Loss: 0.032457951456308365\n",
            "Epoch 39/200, Loss: 0.03536101430654526\n",
            "Epoch 40/200, Loss: 0.026186680421233177\n",
            "Epoch 41/200, Loss: 0.04057233780622482\n",
            "Epoch 42/200, Loss: 0.03090188466012478\n",
            "Epoch 43/200, Loss: 0.020820733159780502\n",
            "Epoch 44/200, Loss: 0.026862910017371178\n",
            "Epoch 45/200, Loss: 0.04225535690784454\n",
            "Epoch 46/200, Loss: 0.0191496592015028\n",
            "Epoch 47/200, Loss: 0.01696610078215599\n",
            "Epoch 48/200, Loss: 0.012816071510314941\n",
            "Epoch 49/200, Loss: 0.017115354537963867\n",
            "Epoch 50/200, Loss: 0.027948400005698204\n",
            "Epoch 51/200, Loss: 0.02706756629049778\n",
            "Epoch 52/200, Loss: 0.020294170826673508\n",
            "Epoch 53/200, Loss: 0.012422643601894379\n",
            "Epoch 54/200, Loss: 0.017336826771497726\n",
            "Epoch 55/200, Loss: 0.02652544155716896\n",
            "Epoch 56/200, Loss: 0.01919446513056755\n",
            "Epoch 57/200, Loss: 0.020731590688228607\n",
            "Epoch 58/200, Loss: 0.010099571198225021\n",
            "Epoch 59/200, Loss: 0.015572233125567436\n",
            "Epoch 60/200, Loss: 0.018974700942635536\n",
            "Epoch 61/200, Loss: 0.01330480445176363\n",
            "Epoch 62/200, Loss: 0.011307906359434128\n",
            "Epoch 63/200, Loss: 0.012522630393505096\n",
            "Epoch 64/200, Loss: 0.01314932107925415\n",
            "Epoch 65/200, Loss: 0.014370116405189037\n",
            "Epoch 66/200, Loss: 0.013429882004857063\n",
            "Epoch 67/200, Loss: 0.012654921039938927\n",
            "Epoch 68/200, Loss: 0.016205260530114174\n",
            "Epoch 69/200, Loss: 0.013500092551112175\n",
            "Epoch 70/200, Loss: 0.010016030631959438\n",
            "Epoch 71/200, Loss: 0.008524175733327866\n",
            "Epoch 72/200, Loss: 0.008202205412089825\n",
            "Epoch 73/200, Loss: 0.02564944699406624\n",
            "Epoch 74/200, Loss: 0.021183570846915245\n",
            "Epoch 75/200, Loss: 0.007399476133286953\n",
            "Epoch 76/200, Loss: 0.013171118684113026\n",
            "Epoch 77/200, Loss: 0.012162155471742153\n",
            "Epoch 78/200, Loss: 0.015602183528244495\n",
            "Epoch 79/200, Loss: 0.015536824241280556\n",
            "Epoch 80/200, Loss: 0.006624175701290369\n",
            "Epoch 81/200, Loss: 0.00675435084849596\n",
            "Epoch 82/200, Loss: 0.01253102719783783\n",
            "Epoch 83/200, Loss: 0.010171624831855297\n",
            "Epoch 84/200, Loss: 0.014730982482433319\n",
            "Epoch 85/200, Loss: 0.007300445344299078\n",
            "Epoch 86/200, Loss: 0.008056762628257275\n",
            "Epoch 87/200, Loss: 0.014828327111899853\n",
            "Epoch 88/200, Loss: 0.006847873330116272\n",
            "Epoch 89/200, Loss: 0.01199667900800705\n",
            "Epoch 90/200, Loss: 0.011125344783067703\n",
            "Epoch 91/200, Loss: 0.006104149390012026\n",
            "Epoch 92/200, Loss: 0.012215939350426197\n",
            "Epoch 93/200, Loss: 0.018945951014757156\n",
            "Epoch 94/200, Loss: 0.0033302889205515385\n",
            "Epoch 95/200, Loss: 0.018960297107696533\n",
            "Epoch 96/200, Loss: 0.0038124844431877136\n",
            "Epoch 97/200, Loss: 0.014864462427794933\n",
            "Epoch 98/200, Loss: 0.008917787112295628\n",
            "Epoch 99/200, Loss: 0.004734749905765057\n",
            "Epoch 100/200, Loss: 0.007216204889118671\n",
            "Epoch 101/200, Loss: 0.015036916360259056\n",
            "Epoch 102/200, Loss: 0.006995877716690302\n",
            "Epoch 103/200, Loss: 0.011693894863128662\n",
            "Epoch 104/200, Loss: 0.010715808719396591\n",
            "Epoch 105/200, Loss: 0.003183352993801236\n",
            "Epoch 106/200, Loss: 0.0060536800883710384\n",
            "Epoch 107/200, Loss: 0.011761030182242393\n",
            "Epoch 108/200, Loss: 0.007118743844330311\n",
            "Epoch 109/200, Loss: 0.003726602764800191\n",
            "Epoch 110/200, Loss: 0.004315904341638088\n",
            "Epoch 111/200, Loss: 0.004559622146189213\n",
            "Epoch 112/200, Loss: 0.012395476922392845\n",
            "Epoch 113/200, Loss: 0.007762380875647068\n",
            "Epoch 114/200, Loss: 0.005709927529096603\n",
            "Epoch 115/200, Loss: 0.005175421945750713\n",
            "Epoch 116/200, Loss: 0.0055058421567082405\n",
            "Epoch 117/200, Loss: 0.005315504968166351\n",
            "Epoch 118/200, Loss: 0.004867300856858492\n",
            "Epoch 119/200, Loss: 0.006479170173406601\n",
            "Epoch 120/200, Loss: 0.008593743667006493\n",
            "Epoch 121/200, Loss: 0.003879907773807645\n",
            "Epoch 122/200, Loss: 0.012700816616415977\n",
            "Epoch 123/200, Loss: 0.005851666443049908\n",
            "Epoch 124/200, Loss: 0.008531289175152779\n",
            "Epoch 125/200, Loss: 0.005302442237734795\n",
            "Epoch 126/200, Loss: 0.006011010613292456\n",
            "Epoch 127/200, Loss: 0.006714025046676397\n",
            "Epoch 128/200, Loss: 0.008686792105436325\n",
            "Epoch 129/200, Loss: 0.010847784578800201\n",
            "Epoch 130/200, Loss: 0.0045501175336539745\n",
            "Epoch 131/200, Loss: 0.003912314306944609\n",
            "Epoch 132/200, Loss: 0.0074714175425469875\n",
            "Epoch 133/200, Loss: 0.008464029058814049\n",
            "Epoch 134/200, Loss: 0.009564192965626717\n",
            "Epoch 135/200, Loss: 0.013147836551070213\n",
            "Epoch 136/200, Loss: 0.004512900020927191\n",
            "Epoch 137/200, Loss: 0.006188543979078531\n",
            "Epoch 138/200, Loss: 0.005570895038545132\n",
            "Epoch 139/200, Loss: 0.007348175160586834\n",
            "Epoch 140/200, Loss: 0.005732013378292322\n",
            "Epoch 141/200, Loss: 0.004130721557885408\n",
            "Epoch 142/200, Loss: 0.0016262091230601072\n",
            "Epoch 143/200, Loss: 0.003603865159675479\n",
            "Epoch 144/200, Loss: 0.0017309043323621154\n",
            "Epoch 145/200, Loss: 0.009374143555760384\n",
            "Epoch 146/200, Loss: 0.007847280241549015\n",
            "Epoch 147/200, Loss: 0.0019230570178478956\n",
            "Epoch 148/200, Loss: 0.014558439143002033\n",
            "Epoch 149/200, Loss: 0.003924477379769087\n",
            "Epoch 150/200, Loss: 0.009041932411491871\n",
            "Epoch 151/200, Loss: 0.007171942852437496\n",
            "Epoch 152/200, Loss: 0.0026420438662171364\n",
            "Epoch 153/200, Loss: 0.005052516702562571\n",
            "Epoch 154/200, Loss: 0.004319159314036369\n",
            "Epoch 155/200, Loss: 0.0034725337754935026\n",
            "Epoch 156/200, Loss: 0.003846134524792433\n",
            "Epoch 157/200, Loss: 0.0015654362505301833\n",
            "Epoch 158/200, Loss: 0.004322835244238377\n",
            "Epoch 159/200, Loss: 0.001650458900257945\n",
            "Epoch 160/200, Loss: 0.004212528932839632\n",
            "Epoch 161/200, Loss: 0.008106363005936146\n",
            "Epoch 162/200, Loss: 0.0037963681388646364\n",
            "Epoch 163/200, Loss: 0.002282538916915655\n",
            "Epoch 164/200, Loss: 0.0029945040587335825\n",
            "Epoch 165/200, Loss: 0.002097277669236064\n",
            "Epoch 166/200, Loss: 0.002417940180748701\n",
            "Epoch 167/200, Loss: 0.0023906356655061245\n",
            "Epoch 168/200, Loss: 0.004875550977885723\n",
            "Epoch 169/200, Loss: 0.002746368758380413\n",
            "Epoch 170/200, Loss: 0.004347079433500767\n",
            "Epoch 171/200, Loss: 0.0011180192232131958\n",
            "Epoch 172/200, Loss: 0.001793006667867303\n",
            "Epoch 173/200, Loss: 0.014395329169929028\n",
            "Epoch 174/200, Loss: 0.0010455565061420202\n",
            "Epoch 175/200, Loss: 0.0104595348238945\n",
            "Epoch 176/200, Loss: 0.0015141929034143686\n",
            "Epoch 177/200, Loss: 0.002737846691161394\n",
            "Epoch 178/200, Loss: 0.0054071154445409775\n",
            "Epoch 179/200, Loss: 0.00678359717130661\n",
            "Epoch 180/200, Loss: 0.0016228985041379929\n",
            "Epoch 181/200, Loss: 0.0041335029527544975\n",
            "Epoch 182/200, Loss: 0.002848571864888072\n",
            "Epoch 183/200, Loss: 0.006373855285346508\n",
            "Epoch 184/200, Loss: 0.002230485202744603\n",
            "Epoch 185/200, Loss: 0.003010682761669159\n",
            "Epoch 186/200, Loss: 0.0015631411224603653\n",
            "Epoch 187/200, Loss: 0.0025697792880237103\n",
            "Epoch 188/200, Loss: 0.0032219321001321077\n",
            "Epoch 189/200, Loss: 0.003932382445782423\n",
            "Epoch 190/200, Loss: 0.0020588752813637257\n",
            "Epoch 191/200, Loss: 0.0016368941869586706\n",
            "Epoch 192/200, Loss: 0.002132229972630739\n",
            "Epoch 193/200, Loss: 0.00194671587087214\n",
            "Epoch 194/200, Loss: 0.0026057474315166473\n",
            "Epoch 195/200, Loss: 0.0031620231457054615\n",
            "Epoch 196/200, Loss: 0.003010708373039961\n",
            "Epoch 197/200, Loss: 0.002565775066614151\n",
            "Epoch 198/200, Loss: 0.0010454206494614482\n",
            "Epoch 199/200, Loss: 0.0011286379303783178\n",
            "Epoch 200/200, Loss: 0.0021931175142526627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test.long()).squeeze()\n",
        "  y_pred = (y_pred > 0.5).float()\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmOm94oGOm56",
        "outputId": "5d2abd68-636d-419e-946b-6b735cef0922"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.4634146341463415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for overfitting\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test.long()).squeeze()\n",
        "  y_pred = (y_pred > 0.5).float()\n",
        "\n",
        "  y_pred_training = model(X_train.long()).squeeze()\n",
        "  y_pred_training = (y_pred_training > 0.5).float()\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {accuracy}\")\n",
        "\n",
        "accuracy_train = accuracy_score(y_train, y_pred_training)\n",
        "print(f\"Train accuracy: {accuracy_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xgemW1iPUrb",
        "outputId": "2d050b71-1ed9-45e0-83f7-68b6d3b4605e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.4634146341463415\n",
            "Train accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [WARNING] Model overfitted.\n"
      ],
      "metadata": {
        "id": "T3B8jCzWQuGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "number_epochs = 20\n",
        "for epoch in range(number_epochs):\n",
        "  optimizer.zero_grad()\n",
        "  output = model(X_train.long()).squeeze()\n",
        "  loss = criterion(output, y_train.float())\n",
        "  loss.backward() # calculate gradient\n",
        "  optimizer.step() # update parameters\n",
        "  print(f\"Epoch {epoch+1}/{number_epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztgf_SLmRJTp",
        "outputId": "041bdf96-d59e-448d-c948-bb24020b0aa2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.0021384635474532843\n",
            "Epoch 2/20, Loss: 0.0033760752994567156\n",
            "Epoch 3/20, Loss: 0.0030223135836422443\n",
            "Epoch 4/20, Loss: 0.0029184434097260237\n",
            "Epoch 5/20, Loss: 0.0005502892890945077\n",
            "Epoch 6/20, Loss: 0.0010230159386992455\n",
            "Epoch 7/20, Loss: 0.0009230072610080242\n",
            "Epoch 8/20, Loss: 0.00613604998216033\n",
            "Epoch 9/20, Loss: 0.0028322588186711073\n",
            "Epoch 10/20, Loss: 0.001301242271438241\n",
            "Epoch 11/20, Loss: 0.0010915573220700026\n",
            "Epoch 12/20, Loss: 0.005572989117354155\n",
            "Epoch 13/20, Loss: 0.003008696250617504\n",
            "Epoch 14/20, Loss: 0.0013568365247920156\n",
            "Epoch 15/20, Loss: 0.0016985323745757341\n",
            "Epoch 16/20, Loss: 0.0008575312094762921\n",
            "Epoch 17/20, Loss: 0.001029003644362092\n",
            "Epoch 18/20, Loss: 0.0010466398671269417\n",
            "Epoch 19/20, Loss: 0.0025451716501265764\n",
            "Epoch 20/20, Loss: 0.0036776098422706127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for overfitting\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  y_pred = model(X_test.long()).squeeze()\n",
        "  y_pred = (y_pred > 0.5).float()\n",
        "\n",
        "  y_pred_training = model(X_train.long()).squeeze()\n",
        "  y_pred_training = (y_pred_training > 0.5).float()\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {accuracy}\")\n",
        "\n",
        "accuracy_train = accuracy_score(y_train, y_pred_training)\n",
        "print(f\"Train accuracy: {accuracy_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPthSXTERKfC",
        "outputId": "b1b8a70b-1351-416a-faef-a00fb61c3487"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.4634146341463415\n",
            "Train accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO = number of epoch and add new sentences to data."
      ],
      "metadata": {
        "id": "K4tRxvevRZt6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}