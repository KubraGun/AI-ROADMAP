{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook/code has been developed based on the academic publication titled  \n",
        "**‚ÄúFine-tuning a local LLaMA-3 large language model for automated privacy-preserving generation of patient letters in oncology.‚Äù**\n",
        "\n",
        "The implementation replicates the methods and configurations described in the paper using QLoRA fine-tuning for structured clinical data, specifically aiming to generate physician-style medical letters from structured oncology case inputs.\n",
        "\n",
        "This includes:\n",
        "- Model quantization with 4-bit QLoRA (NF4)\n",
        "- LoRA configuration (rank, scaling, dropout) as described in the paper\n",
        "- Training parameters, optimizer choice, and target modules matching the original methodology\n",
        "\n",
        "üí° The implementation is intended for **academic and non-commercial use** only, in the context of methodological exploration and reproduction of the paper‚Äôs pipeline.\n",
        "\n",
        "üìé Reference:  \n",
        "Hou Y, Bert C, Gomaa A, Lahmer G, H√∂fler D,\n",
        "Weissmann T, Voigt R, Schubert P,\n",
        "Schmitter C, Depardon A, Semrau S, Maier A,\n",
        "Fietkau R, Huang Y and Putz F (2025)\n",
        "Fine-tuning a local LLaMA-3 large language\n",
        "model for automated privacy-preserving\n",
        "physician letter generation in radiation\n",
        "oncology. Front. Artif. Intell. 7:1493716.\n",
        "doi: 10.3389/frai.2024.1493716\n",
        "\n",
        "All original research rights belong to the authors of the referenced paper."
      ],
      "metadata": {
        "id": "ZGx8SWrb_SIP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP6QRRWP_PCB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        ")\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 1. MODEL VE G√ñREV AYARLARI\n",
        "# ==========================\n",
        "MODEL_NAME = \"meta-llama/Llama-3-8b-hf\"  # alternatif: \"meta-llama/Llama-2-13b-hf\"\n",
        "\n",
        "if \"13b\" in MODEL_NAME.lower():\n",
        "    MAX_SEQ_LENGTH = 1500  # Makale: summary task i√ßin\n",
        "    TOTAL_STEPS = 500\n",
        "    TASK = \"patient_case_summarization\"\n",
        "else:\n",
        "    MAX_SEQ_LENGTH = 2000  # Makale: physician letter generation\n",
        "    TOTAL_STEPS = 15000\n",
        "    TASK = \"physician_letter_generation\"\n",
        "\n",
        "OUTPUT_DIR = f\"./qlora_{MODEL_NAME.split('/')[-1]}_{TASK}\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "2cAZDCYJ_1DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 2. QLoRA (4-BIT) QUANTIZATION\n",
        "# ==========================\n",
        "\n",
        "# QLoRA = LoRA + GPU Conf.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Makale: QLoRA ‚Üí 4-bit\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",  # Daha stabil ve y√ºksek performanslƒ± quantization\n",
        ")"
      ],
      "metadata": {
        "id": "u1q6VHdEADD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. MODEL & TOKENIZER\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n"
      ],
      "metadata": {
        "id": "g_cEHFqzAMiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. LoRA KONFƒ∞G√úRASYONU (Makale ile birebir)\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)  # LayerNorm freeze, cast output, vb.\n",
        "\n",
        "target_modules = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
        "]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,                      # Makale: LoRA rank\n",
        "    lora_alpha=64,             # Makale: scaling factor\n",
        "    target_modules=target_modules,\n",
        "    lora_dropout=0.05,         # Makale: dropout\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "DR8ZvZrQArF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. VERƒ∞ SETƒ∞ Y√úKLEME\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset(\"json\", data_files={\"train\": \"your_data.jsonl\"})[\"train\"]\n",
        "\n",
        "def format_to_text(example):\n",
        "    return {\n",
        "        \"text\": example[\"input\"] + \"\\n\\n\" + example[\"output\"]\n",
        "    }\n",
        "\n",
        "formatted_dataset = raw_dataset.map(format_to_text)"
      ],
      "metadata": {
        "id": "7iajz36pA1HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Eƒûƒ∞Tƒ∞M PARAMETRELERƒ∞\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,       # Makale: 2\n",
        "    gradient_accumulation_steps=2,       # Makale: 2\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_8bit\",            # Makale: \"8-bit paged AdamW\"\n",
        "    learning_rate=1e-5,                  # Makale: 1e-5\n",
        "    max_steps=TOTAL_STEPS,               # Makale: 500 veya 15000\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "    fp16=True,\n",
        "    report_to=\"tensorboard\",\n",
        "    ddp_find_unused_parameters=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "DFWs1apqA6sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=lora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "XEmmXsi0A__Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(f\"{OUTPUT_DIR}/final_model\")\n",
        "print(f\"\\n‚úÖ Eƒüitim tamamlandƒ±. Model burada kaydedildi: {OUTPUT_DIR}/final_model\")\n"
      ],
      "metadata": {
        "id": "1qpnNPGxTY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model fine tune i≈ülemi burada tamamlandƒ±. sƒ±rada deƒüerlendirmesi var\n",
        "model_path = \"./qlora_Llama-3-8b-hf_physician_letter_generation/final_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(\"cuda\")"
      ],
      "metadata": {
        "id": "KzUYJWsAT-1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_input = \"\"\"Diagnosis: Breast cancer\n",
        "History: Chemotherapy completed in May\n",
        "Follow-up: MRI in 3 months\"\"\""
      ],
      "metadata": {
        "id": "4IbDta8HUIy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# yeni girdinin √∂n i≈ülemesini yap:\n",
        "import torch\n",
        "inputs = tokenizer(new_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "Q27fMM8WUJf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "rouge = load(\"rouge\")\n",
        "scores = rouge.compute(predictions=[output_text], references=[\"Senin referans doktor mektubun\"])\n",
        "print(scores)\n"
      ],
      "metadata": {
        "id": "Xj6zkaJBURU5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}