{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f17d4a4e33c4770b1fa1571f37527a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6291ffe371140fca5f395b00dd7ffdb",
              "IPY_MODEL_31ac2dbc59b64389baa68d509a2066f4",
              "IPY_MODEL_d0c4dfb562884d31a3dd5b5d037c9333"
            ],
            "layout": "IPY_MODEL_0bbf2b0b6a6b4c6a9ad9af049c2c6eae"
          }
        },
        "a6291ffe371140fca5f395b00dd7ffdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d318aade2c2e4373835380fdecec7a39",
            "placeholder": "​",
            "style": "IPY_MODEL_1fc180edd12244fc92240e62e5dec80f",
            "value": "(…)t_Training_Dataset_27K_responses-v11.csv: 100%"
          }
        },
        "31ac2dbc59b64389baa68d509a2066f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f482568138894dc4b183a39c702f2d14",
            "max": 19202474,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bd73c60dc7a4194a17c3303ee073bdd",
            "value": 19202474
          }
        },
        "d0c4dfb562884d31a3dd5b5d037c9333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7cf234e2f064ea6858c5539125be04c",
            "placeholder": "​",
            "style": "IPY_MODEL_7527155aaad64757b2ebaa22b8aa00bb",
            "value": " 19.2M/19.2M [00:00&lt;00:00, 35.9MB/s]"
          }
        },
        "0bbf2b0b6a6b4c6a9ad9af049c2c6eae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d318aade2c2e4373835380fdecec7a39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fc180edd12244fc92240e62e5dec80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f482568138894dc4b183a39c702f2d14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd73c60dc7a4194a17c3303ee073bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7cf234e2f064ea6858c5539125be04c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7527155aaad64757b2ebaa22b8aa00bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b70db5baea4915a620f3d2f91c52dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f61b031231a14d8ebd30679115ec42fa",
              "IPY_MODEL_462fe57dea114535980b57e0dee6eaa2",
              "IPY_MODEL_2789074c2a194c1c8933b417a879369e"
            ],
            "layout": "IPY_MODEL_5b72c5e42f2742fba70ad5df0c08fff0"
          }
        },
        "f61b031231a14d8ebd30679115ec42fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e69a64d6c5649bfaa33d9b8546a136e",
            "placeholder": "​",
            "style": "IPY_MODEL_55c311652c794d609ebdb74c63fc0c58",
            "value": "Generating train split: 100%"
          }
        },
        "462fe57dea114535980b57e0dee6eaa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca95db1d34a04829b40804dcdd480c43",
            "max": 26872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97cceb9cfd1942438c6d16d8c3aab633",
            "value": 26872
          }
        },
        "2789074c2a194c1c8933b417a879369e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35170e51ee844d6aaf83c8915a3c4fc3",
            "placeholder": "​",
            "style": "IPY_MODEL_7390f9f7efee4a06b63d38d296db5996",
            "value": " 26872/26872 [00:00&lt;00:00, 68311.38 examples/s]"
          }
        },
        "5b72c5e42f2742fba70ad5df0c08fff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e69a64d6c5649bfaa33d9b8546a136e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55c311652c794d609ebdb74c63fc0c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca95db1d34a04829b40804dcdd480c43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97cceb9cfd1942438c6d16d8c3aab633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35170e51ee844d6aaf83c8915a3c4fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7390f9f7efee4a06b63d38d296db5996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82d4659f76cf49339e6199107a1e62f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de34fdc22faa40f880186869d080975d",
              "IPY_MODEL_41598ae7a167449696f34f2043540c46",
              "IPY_MODEL_1b724bc0214e43668cdc0328424354c2"
            ],
            "layout": "IPY_MODEL_c14bc8d531c04026aa8ac3271df60da4"
          }
        },
        "de34fdc22faa40f880186869d080975d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db6d599d1b64ae7a02e9602be430578",
            "placeholder": "​",
            "style": "IPY_MODEL_7feab8951b8045f4bd5c4f2fb767c84b",
            "value": "Map: 100%"
          }
        },
        "41598ae7a167449696f34f2043540c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86956be9a9a24470a7e8a53f369cfd8c",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76bdeb1cad9b48afa45955ed002beaed",
            "value": 1000
          }
        },
        "1b724bc0214e43668cdc0328424354c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_387426dbbd844ca9939528363abb9e38",
            "placeholder": "​",
            "style": "IPY_MODEL_4b2c3866f3c54c9dab403f833311a752",
            "value": " 1000/1000 [00:00&lt;00:00, 4935.53 examples/s]"
          }
        },
        "c14bc8d531c04026aa8ac3271df60da4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7db6d599d1b64ae7a02e9602be430578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7feab8951b8045f4bd5c4f2fb767c84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86956be9a9a24470a7e8a53f369cfd8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76bdeb1cad9b48afa45955ed002beaed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "387426dbbd844ca9939528363abb9e38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b2c3866f3c54c9dab403f833311a752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f54367b54d1d467bbb2d2112d3fe7ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b406f60bbf5b43c184932157b522f79f",
              "IPY_MODEL_a5a4e769992740d99f342270e4549536",
              "IPY_MODEL_0e081f058e9d41c6ac7f7ed667011c5e"
            ],
            "layout": "IPY_MODEL_9f3ef316df134ccb86afd8e0e7aab781"
          }
        },
        "b406f60bbf5b43c184932157b522f79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d38725246f3140c4b5510e0cae5fca29",
            "placeholder": "​",
            "style": "IPY_MODEL_6ae41f71b3de47e9b8f1078cb7a4cf02",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "a5a4e769992740d99f342270e4549536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e859a3dcd28442eb3c85fc2b403bb04",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a878b924b864d2ea1058bec8c14f9c1",
            "value": 1000
          }
        },
        "0e081f058e9d41c6ac7f7ed667011c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25f509f747dd46fcb1632f071039b231",
            "placeholder": "​",
            "style": "IPY_MODEL_8f77f6ef10684418a5d18b8c9150dcf0",
            "value": " 1000/1000 [00:00&lt;00:00, 28562.80 examples/s]"
          }
        },
        "9f3ef316df134ccb86afd8e0e7aab781": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d38725246f3140c4b5510e0cae5fca29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae41f71b3de47e9b8f1078cb7a4cf02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e859a3dcd28442eb3c85fc2b403bb04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a878b924b864d2ea1058bec8c14f9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25f509f747dd46fcb1632f071039b231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f77f6ef10684418a5d18b8c9150dcf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 📌 Bu not defterinde yer alan içerikler, ağırlıklı olarak DataCamp platformundaki ilgili eğitim modülleri temel alınarak hazırlanmıştır.\n",
        "# Ancak içerik yalnızca DataCamp ile sınırlı değildir; Hugging Face dokümantasyonları, PEFT kütüphanesi belgeleri,\n",
        "# arXiv akademik makaleleri, GitHub açık kaynak projeleri, Stack Overflow, Medium yazıları ve çeşitli teknik bloglar dahil olmak üzere\n",
        "# birçok farklı platform ve kaynaktan edinilen bilgilerle zenginleştirilmiştir.\n",
        "#\n",
        "# Amaç, eğitim sürecimi daha derinlemesine ve çok yönlü bir şekilde desteklemek; konuyu yalnızca tek bir kaynaktan değil,\n",
        "# farklı bakış açılarıyla öğrenerek kalıcı bir bilgi temeli oluşturmaktır.\n",
        "#\n",
        "# Bu içerikler, herhangi bir ticari amaç taşımadan, yalnızca kişisel öğrenim, uygulama ve arşivleme amacıyla kullanılmaktadır.\n",
        "# Telif hakkı sahiplerine saygı gereği, orijinal kaynaklara doğrudan atıf yapılmaya özen gösterilmiştir. 🙏\n",
        "#\n",
        "# Eğitim kapsamında verilen görevleri kendi öğrenim sürecim için yeniden uyguluyorum.\n",
        "# Orijinal senaryo ve veri yapısı ilgili platforma aittir."
      ],
      "metadata": {
        "id": "JSBRgsZp1Amj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://campus.datacamp.com/courses/fine-tuning-with-llama-3"
      ],
      "metadata": {
        "id": "S67t7hTw4bbB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aHfzMhUa4Vsf",
        "outputId": "8162caea-2e4b-40ab-9bd3-ded979735dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtune\n",
            "  Downloading torchtune-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting torchdata==0.11.0 (from torchtune)\n",
            "  Downloading torchdata-0.11.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting datasets (from torchtune)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.30.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.3)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.11)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
            "Collecting tiktoken (from torchtune)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting blobfile>=2 (from torchtune)\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.21.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
            "Collecting omegaconf (from torchtune)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (5.9.5)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.1.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.11.0->torchtune) (2.6.0+cu124)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile>=2->torchtune)\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.1)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->torchtune)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.2)\n",
            "Collecting xxhash (from datasets->torchtune)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->torchtune)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.13.1)\n",
            "Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]->torchtune)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->torchtune)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.11.0->torchtune) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2->torchdata==0.11.0->torchtune)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.11.0->torchtune) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchdata==0.11.0->torchtune) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.11.0->torchtune) (3.0.2)\n",
            "Downloading torchtune-0.6.1-py3-none-any.whl (910 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m910.7/910.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.11.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=816c10472bad515e13edfa5484cef7c626734346bb52dfef04bada6e12bd38b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, xxhash, pycryptodomex, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-transfer, fsspec, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, blobfile, nvidia-cusolver-cu12, datasets, torchdata, torchtune\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 blobfile-3.0.0 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 hf-transfer-0.1.9 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 pycryptodomex-3.22.0 tiktoken-0.9.0 torchdata-0.11.0 torchtune-0.6.1 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "0db1332fa04d4dad9dc2a907206937ce"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install torchtune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! tune ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MmXVqo4k4f8B",
        "outputId": "f274dea7-39fc-4930-95c2-0998cc79770a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/__init__.py\", line 16, in <module>\n",
            "    import torchao  # noqa\n",
            "    ^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'torchao'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tune\", line 5, in <module>\n",
            "    from torchtune._cli.tune import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/__init__.py\", line 18, in <module>\n",
            "    raise ImportError(\n",
            "ImportError: \n",
            "        torchao not installed.\n",
            "        Please follow the instructions at https://pytorch.org/torchtune/main/install.html#pre-requisites\n",
            "        to install torchao.\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "ELD9dlx48qqh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('bitext/Bitext-customer-support-llm-chatbot-training-dataset', split='train')\n",
        "# Yüklendikten sonra, veri kümesinin talimat ve yanıt gibi görevimizle alakalı sütunlarını görebiliriz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "8f17d4a4e33c4770b1fa1571f37527a5",
            "a6291ffe371140fca5f395b00dd7ffdb",
            "31ac2dbc59b64389baa68d509a2066f4",
            "d0c4dfb562884d31a3dd5b5d037c9333",
            "0bbf2b0b6a6b4c6a9ad9af049c2c6eae",
            "d318aade2c2e4373835380fdecec7a39",
            "1fc180edd12244fc92240e62e5dec80f",
            "f482568138894dc4b183a39c702f2d14",
            "6bd73c60dc7a4194a17c3303ee073bdd",
            "e7cf234e2f064ea6858c5539125be04c",
            "7527155aaad64757b2ebaa22b8aa00bb",
            "55b70db5baea4915a620f3d2f91c52dd",
            "f61b031231a14d8ebd30679115ec42fa",
            "462fe57dea114535980b57e0dee6eaa2",
            "2789074c2a194c1c8933b417a879369e",
            "5b72c5e42f2742fba70ad5df0c08fff0",
            "5e69a64d6c5649bfaa33d9b8546a136e",
            "55c311652c794d609ebdb74c63fc0c58",
            "ca95db1d34a04829b40804dcdd480c43",
            "97cceb9cfd1942438c6d16d8c3aab633",
            "35170e51ee844d6aaf83c8915a3c4fc3",
            "7390f9f7efee4a06b63d38d296db5996"
          ]
        },
        "id": "JHpwT7SFNHx-",
        "outputId": "2057c28f-4a36-472b-ce65-8da5cd7e676f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)t_Training_Dataset_27K_responses-v11.csv:   0%|          | 0.00/19.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f17d4a4e33c4770b1fa1571f37527a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/26872 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55b70db5baea4915a620f3d2f91c52dd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.column_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTXZUAm5Rw4E",
        "outputId": "ff4b22c4-ea5f-4d9d-cc58-a24b5b6b5ff7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['flags', 'instruction', 'category', 'intent', 'response']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint"
      ],
      "metadata": {
        "id": "60wy1DtDR_rk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(ds[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKf7cvR7S8ds",
        "outputId": "3af34031-b8c5-4485-9139-e928320771ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'category': 'ORDER',\n",
            " 'flags': 'B',\n",
            " 'instruction': 'question about cancelling order {{Order Number}}',\n",
            " 'intent': 'cancel_order',\n",
            " 'response': \"I've understood you have a question regarding canceling order \"\n",
            "             \"{{Order Number}}, and I'm here to provide you with the \"\n",
            "             'information you need. Please go ahead and ask your question, and '\n",
            "             \"I'll do my best to assist you.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APMP7iIMS-Q8",
        "outputId": "d803b874-ab94-4608-f9b1-bf118cc53c82"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(26872, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_thousand_points = ds[:1000]\n",
        "ds = Dataset.from_dict(first_thousand_points)"
      ],
      "metadata": {
        "id": "OtAZjd-pRxBW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG64xPrekWcv",
        "outputId": "a315537f-b987-47e2-dcac-f2a2d97853a2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['flags', 'instruction', 'category', 'intent', 'response'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_example(row):\n",
        "  row['conversation'] = f\"Query: {row['instruction']}\\nResponse: {row['response']}\"\n",
        "  return row\n",
        "\n",
        "ds = ds.map(merge_example)\n",
        "print(ds[0]['conversation'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "82d4659f76cf49339e6199107a1e62f7",
            "de34fdc22faa40f880186869d080975d",
            "41598ae7a167449696f34f2043540c46",
            "1b724bc0214e43668cdc0328424354c2",
            "c14bc8d531c04026aa8ac3271df60da4",
            "7db6d599d1b64ae7a02e9602be430578",
            "7feab8951b8045f4bd5c4f2fb767c84b",
            "86956be9a9a24470a7e8a53f369cfd8c",
            "76bdeb1cad9b48afa45955ed002beaed",
            "387426dbbd844ca9939528363abb9e38",
            "4b2c3866f3c54c9dab403f833311a752"
          ]
        },
        "id": "RwCAke00khCf",
        "outputId": "5789be78-b866-4a2a-8c13-b109778a290e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82d4659f76cf49339e6199107a1e62f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: question about cancelling order {{Order Number}}\n",
            "Response: I've understood you have a question regarding canceling order {{Order Number}}, and I'm here to provide you with the information you need. Please go ahead and ask your question, and I'll do my best to assist you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds.save_to_disk('preprocessed_dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "f54367b54d1d467bbb2d2112d3fe7ba9",
            "b406f60bbf5b43c184932157b522f79f",
            "a5a4e769992740d99f342270e4549536",
            "0e081f058e9d41c6ac7f7ed667011c5e",
            "9f3ef316df134ccb86afd8e0e7aab781",
            "d38725246f3140c4b5510e0cae5fca29",
            "6ae41f71b3de47e9b8f1078cb7a4cf02",
            "2e859a3dcd28442eb3c85fc2b403bb04",
            "4a878b924b864d2ea1058bec8c14f9c1",
            "25f509f747dd46fcb1632f071039b231",
            "8f77f6ef10684418a5d18b8c9150dcf0"
          ]
        },
        "id": "Y-k8_psnlwO3",
        "outputId": "a4987ec3-7561-4185-faac-c4ef4754fa53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f54367b54d1d467bbb2d2112d3fe7ba9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk"
      ],
      "metadata": {
        "id": "o0ayG2FvmKIX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_preprocessed = load_from_disk('preprocessed_dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "B8Xl-nqimbkv",
        "outputId": "94376726-0023-4e5e-be2e-34491ddb943d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'load_from_disk' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-68fb0ff534e9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_preprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessed_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_from_disk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_preprocessed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9Ryvpk9mdqI",
        "outputId": "5fa6849f-60b8-4612-8e2a-d0080364d488"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['flags', 'instruction', 'category', 'intent', 'response', 'conversation'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_from_disk('preprocessed_dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "mZO91twSmfcH",
        "outputId": "def4a54b-a3c9-449f-adac-c62e9ad94585"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Directory preprocessed_dataset not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3007d6209a0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessed_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_to_fs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Directory {dataset_path} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m     if fs.isfile(posixpath.join(dataset_path, config.DATASET_INFO_FILENAME)) and fs.isfile(\n\u001b[1;32m   2142\u001b[0m         \u001b[0mposixpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET_STATE_JSON_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Directory preprocessed_dataset not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml"
      ],
      "metadata": {
        "id": "f7GP9WFClqX8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config_dict = {\n",
        "    \"batch_size\": 4,\n",
        "    \"device\": \"cuda\",\n",
        "    \"model\" : {\n",
        "        \"_component_\": \"torchtune.models.llama3_2.llama3_2_1b\"\n",
        "    },\n",
        "    #...\n",
        "    }"
      ],
      "metadata": {
        "id": "5l3n2SIUlttF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_file_path = \"custom_recipe.yaml\"\n",
        "with open(yaml_file_path, \"w\") as yaml_file:\n",
        "  yaml.dump(config_dict, yaml_file)"
      ],
      "metadata": {
        "id": "IRIUVLyCmTjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigürasyonlar da hazır olduğuna göre, fine tune başlatılabilir\n",
        "# Aşağıdaki komut, custom_recipe.yaml yapılandırma dosyası ile ince ayar işini doğru şekilde çalıştırır\n",
        "!tune run --config custom_recipe.yaml"
      ],
      "metadata": {
        "id": "j60u4lEUmkLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Fine-tunşng\n",
        "model_name = \"Maykeye/TinyLLama-v0\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Bir cümle bittiğinde belirteç üretimini durdurmak için pad_token'ı cümle sonu\n",
        "# belirtecine ayarlarız\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "FTc4rb7CN2XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments"
      ],
      "metadata": {
        "id": "NkYLPhyFnb3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace'in TrainingArguments yardımcı sınıfı kullanılarak eğitim argümanları yapılandırma:\n",
        "# https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/trainer#transformers.TrainingArguments\n",
        "training_arguments = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=2e-3,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=200,\n",
        "    #...\n",
        "    gradient_accumulation_steps=2,\n",
        "    save_steps=10\n",
        "  )"
      ],
      "metadata": {
        "id": "1JT5_vgqPJBQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFTTrainer'ı kullanmak için, modeli, tokenizer i ve training dataset i\n",
        "# aşağıdakilerle sağlamak gerekir:\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset= dataset,\n",
        "    dataset_text_field=\"conversation\",\n",
        "    args=training_arguments,\n",
        "    max_seq_length=250\n",
        ")"
      ],
      "metadata": {
        "id": "HSosTrPQRFBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eğitim başlatılır\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "DBMsbUxEazdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model ROUGE1 metriği kullanılarak değerlendirilir.\n",
        "# https://huggingface.co/spaces/evaluate-metric/rouge\n",
        "import evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "Db72US59bdFO",
        "outputId": "477c6b06-762e-4558-9fc1-4eba4b90b070"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'evaluate'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-19628169aad2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model ROUGE1 metriği kullanılarak değerlendirilir.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# https://huggingface.co/spaces/evaluate-metric/rouge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "predictions = [\"hello there\", \"general kenobi\"]\n",
        "references = [\"hello there\", \"master yoda\"]"
      ],
      "metadata": {
        "id": "ptUWcIj4d3jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "print(results)\n",
        "# İlk örnek tam eşleşme sağlar bu yüzden 1 puan alır, ikinci örnek eşleşme sağlamaz, 0 puan alır.\n",
        "# Output: {'rouge1': 0.5, 'rouge2': 0,5, 'rougeL': 0.5, 'rougeLsum': 0.5}"
      ],
      "metadata": {
        "id": "mkGNP0okeMRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE-1 nasıl kullanılır?\n",
        "\n",
        "def generate_predictions_and_references(dataset):\n",
        "  \"\"\"\n",
        "    Bu fonksiyon, metni tokenizer.encode ile tokenlaştırır, bu da metni modelin aslında girdi\n",
        "    olarak aldığı sayısal değerler listesine çıktılar üretir. Bu jetonları içeren modelle, bunları sayı\n",
        "    listesinden doğal metne geri dönüştürür.\n",
        "    Ardından referans ve üretilen cevap döndürülür\n",
        "  \"\"\"\n",
        "  predictions = []\n",
        "  references = []\n",
        "\n",
        "  for row in dataset:\n",
        "    inputs = tokenizer.encode(row[\"instruction\"], return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs)\n",
        "    decoded_outputs = tokenizer.decode(outputs[0, inputs.shape[1]:], skip_special_tokens=True)\n",
        "    # predictions.append(decoded_outputs)\n",
        "    # references.append(row[\"response\"])\n",
        "    references += [row[\"response\"]]\n",
        "    predictions += [decoded_outputs]\n",
        "\n",
        "  return predictions, references"
      ],
      "metadata": {
        "id": "sX1LxmXIhz-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references, predictions = generate_predictions_and_references(evaluation_dataset)"
      ],
      "metadata": {
        "id": "LqaouUe6kwIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "results = rouge.compute(predictions=predictions, references=references)"
      ],
      "metadata": {
        "id": "3dlV1VE2k3kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient Fine-tuning with LoRA\n"
      ],
      "metadata": {
        "id": "cTOC2DFC5j3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA’yı uygulamak için parameter-efficient fine-tuning (PEFT) kütüphanesi kullanılır. LoRA ayarları için LoraConfig sınıfı tanımlanır:\n",
        "\n",
        "r: Düşük dereceli matrislerin boyutunu belirler. Küçük r değeri daha az bellek tüketir, büyük r değeri modele daha yakın sonuç verir.\n",
        "\n",
        "lora_alpha: LoRA tarafından öğrenilen değişimlerin orijinal modele ne kadar yansıtılacağını belirler.\n",
        "\n",
        "lora_dropout: LoRA katmanlarından rastgele bırakma (dropout) oranını ayarlar.\n",
        "\n",
        "bias: Bias parametrelerinin eğitilip eğitilmeyeceğini belirler (katman bazlı ya da genel).\n",
        "\n",
        "task_type: Örneğin Llama için causal_lm olarak ayarlanır.\n",
        "\n",
        "target_modules: LoRA’nın uygulanacağı büyük dil modeli (LLM) matrislerini belirler."
      ],
      "metadata": {
        "id": "Ojc56-DF6dse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig"
      ],
      "metadata": {
        "id": "D6L6i8fb6fBq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=12,\n",
        "    lora_alpha=32 # or scaling factor\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "WFE_m3_wAM1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### r değeri\n",
        "\n",
        "Standart aralık: 4 ≤ r ≤ 64\n",
        "\n",
        "En yaygın kullanılan: r = 8\n",
        "\n",
        "Büyük modeller veya hassas görevler: r = 16 ya da r = 32\n",
        "\n",
        "\n",
        "Akademik ve Uygulamalı Kaynaklardan Alıntılar:\n",
        "1. LoRA Orijinal Makalesi (Hu et al., 2021)\n",
        "LoRA: Low-Rank Adaptation of Large Language Models\n",
        "📄 arXiv:2106.09685\n",
        "\n",
        "Çeşitli deneylerde r = 4, 8, 16 değerleri test edilmiştir.\n",
        "\n",
        "r=8 ve lora_alpha=16 kombinasyonu, performans-maliyet dengesi açısından çok başarılıdır.\n",
        "\n",
        "2. Hugging Face PEFT dokümantasyonu\n",
        "https://huggingface.co/docs/peft\n",
        "\n",
        "r parametresi için doğrudan önerilen bir aralık verilmez, ancak örneklerde genellikle 4, 8 veya 16 kullanılır.\n",
        "\n",
        "3. Uygulamalı projelerden örnekler (Open LLM Leaderboard, Alpaca-LoRA, etc.)\n",
        "Alpaca-LoRA: r=8, lora_alpha=16\n",
        "\n",
        "Baize, Vicuna-LoRA, Koala-LoRA: r=8 yaygın\n",
        "\n",
        "Çok büyük modellerde (örneğin LLaMA 65B): r=16 veya r=32 tercih edilebiliyor\n",
        "\n",
        "r Parametresi ve Etkileri:\n",
        "\n",
        "r Değeri\tBellek Kullanımı\tModel Yakınlığı\tKullanım Yaygınlığı\n",
        "4\tÇok düşük\tDüşük\tSık (edge device, hızlı test)\n",
        "8\tDüşük\tDengeli\tEn yaygın\n",
        "16\tOrta\tYüksek\tGelişmiş modeller\n",
        "32+\tYüksek\tÇok yüksek\tBüyük modeller, araştırma amaçlı\n",
        "💡 Notlar:\n",
        "r değeri arttıkça performans artar ama maliyet (VRAM, zaman) da artar.\n",
        "\n",
        "LoRA’nın en güçlü yanı, küçük r değerleriyle dahi etkili sonuçlar verebilmesidir.\n",
        "\n",
        "Uygulamada genellikle lora_alpha = r * 2 şeklinde ayarlanır (örneğin r=8, alpha=16).\n"
      ],
      "metadata": {
        "id": "4DtkiTnvUyuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lora_alpha\n",
        "\n",
        "lora_alpha, LoRA’nın düşük dereceli matrislerinden gelen gradyanların (öğrenme sinyali) orijinal model ağırlıklarına ne kadar etki edeceğini kontrol eden ölçekleme (scaling) katsayısıdır.\n",
        "\n",
        "r → Ne kadar bilgi taşıyoruz? (boyut)\n",
        "\n",
        "lora_alpha → Bu bilgiyi ne kadar güçlü uyguluyoruz? (etki gücü)\n",
        "\n",
        "LoRA’nın temel matematiği şöyle işler:\n",
        "```\n",
        "W + ΔW ≈ W + α/r × (A × B)\n",
        "```\n",
        "\n",
        "W: Orijinal modelin ağırlığı\n",
        "\n",
        "A ve B: LoRA'nın öğrenilen düşük-rank matrisleri\n",
        "\n",
        "α (yani lora_alpha): ölçekleme katsayısı\n",
        "\n",
        "r: rank değeri\n",
        "\n",
        "Yani lora_alpha / r oranı, düşük-rank güncellemesinin ne kadar etkili olacağını belirler.\n",
        "\n",
        "Orijinal LoRA makalesi (Hu et al., 2021):\n",
        "“We set α such that α / r = 1, 8, or 32 in various experiments. We find α = 16 and r = 8 to be a good balance between performance and stability.”\n",
        "\n",
        "Yani önerilen ve sık kullanılan kombinasyonlar şunlar:\n",
        "\n",
        "r = 8, alpha = 16 → α/r = 2 --> en yaygın\n",
        "\n",
        "r = 4, alpha = 16 → α/r = 4\n",
        "\n",
        "r = 16, alpha = 32 → α/r = 2\n",
        "\n",
        "\n",
        "Hugging Face e göre örnek kullanımlar:\n",
        "\n",
        "Alpaca-LoRA: r=8, lora_alpha=16\n",
        "\n",
        "Vicuna-LoRA: r=16, lora_alpha=32\n",
        "\n",
        "LLaMA-LoRA: r=64, lora_alpha=128 veya daha düşük\n",
        "\n",
        "\n",
        "#### lora_alpha Nasıl Seçilir?\n",
        "\n",
        "Amaç / Durum\tr\tlora_alpha\tAçıklama\n",
        "Küçük model, hızlı test\t4\t8\tDaha düşük bellek ve hızlı eğitim\n",
        "Dengeli kullanım (en yaygın)\t8\t16\tStandarttır, çoğu çalışmada bu kullanılır\n",
        "Daha hassas uyum, büyük model\t16\t32 veya 64\tDaha fazla bilgi aktarımı\n",
        "Çok büyük modeller\t64\t128\tAşırı durumlar, dikkatli kullanılmalı\n",
        "\n",
        "| **Yüksek `lora_alpha`**               | **Düşük `lora_alpha`**              |\n",
        "|--------------------------------------|-------------------------------------|\n",
        "| Daha agresif öğrenme                | Daha yumuşak güncellemeler         |\n",
        "| Aşırı uyuma (overfitting) riski     | Daha güvenli ama yavaş öğrenme     |\n",
        "| Büyük modellerde daha etkili        | Küçük modellerde yeterli           |\n",
        "\n",
        "\n",
        "#### İdeal kombinasyon önerisi\n",
        "| **r** | **lora_alpha** | **alpha / r** | **Ne zaman tercih edilir?**              |\n",
        "|------:|----------------:|---------------:|------------------------------------------|\n",
        "| 4     | 8              | 2             | Hızlı test, düşük bellek kullanımı       |\n",
        "| 8     | 16             | 2             | En yaygın kullanım, genel denge          |\n",
        "| 16    | 32             | 2             | Büyük modeller için daha fazla hassasiyet |\n",
        "| 64    | 128            | 2             | Büyük LLM'ler, araştırma amaçlı kullanım |\n"
      ],
      "metadata": {
        "id": "LC12X7cNVs7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lora_dropout\n",
        "\n",
        "lora_dropout, LoRA modülündeki düşük-rank matrislerde uygulanacak dropout oranını belirler. Bu, eğitim sırasında bazı nöronların rastgele devre dışı bırakılmasını sağlar.\n",
        "\n",
        "Dropout, derin öğrenmede aşırı öğrenmeyi (overfitting) önlemek için kullanılan klasik bir teknik olup, LoRA'da da aynı amaçla uygulanır.\n",
        "\n",
        "#### Yaygın Kullanım Aralıkları\n",
        "| **Senaryo**                        | **lora_dropout** | **Açıklama**                                               |\n",
        "|-----------------------------------|------------------|------------------------------------------------------------|\n",
        "| Aşırı küçük veri kümesi           | 0.1 – 0.2         | Aşırı öğrenmeyi önlemek için daha agresif dropout          |\n",
        "| Orta büyüklükte, dengeli veri     | 0.05 – 0.1        | En yaygın kullanım aralığı                                 |\n",
        "| Büyük veri, çok parametreli model | 0.0 – 0.05        | Dropout’a gerek kalmadan genelleme sağlanabilir            |\n",
        "| Sıfır dropout                     | 0.0               | Maksimum öğrenme, ancak overfitting riski olabilir         |\n",
        "\n",
        "\n",
        "#### Etkisi\n",
        "| **Düşük dropout (0.0 – 0.05)**       | **Yüksek dropout (0.1 – 0.2)**            |\n",
        "|-------------------------------------|-------------------------------------------|\n",
        "| Daha iyi doğruluk (accuracy)        | Daha yüksek genelleme (generalization)    |\n",
        "| Aşırı öğrenme riski                 | Öğrenme süresi artabilir                  |\n",
        "| Büyük modeller için uygun           | Küçük veri setleri için daha güvenli      |\n",
        "\n",
        "\n",
        "- **Varsayılan öneri**: `lora_dropout = 0.05`\n",
        "- Küçük veri setlerinde veya çok hızlı öğrenen modellerde: `0.1`\n",
        "- Çok büyük, zengin veri setleri için (örneğin 1M+ örnek): `0.0 – 0.01`\n",
        "\n",
        "https://huggingface.co/docs/peft:\n",
        "\n",
        "\"lora_dropout\" is a dropout probability to apply to the LoRA layers during training. Defaults to 0.0, but values like 0.05 – 0.1 are common for regularization.\n"
      ],
      "metadata": {
        "id": "PqHpBCKsY9qw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### bias\n",
        "bias parametresi, orijinal modeldeki bias (sapma) terimlerinin ince ayar sırasında eğitilip eğitilmeyeceğini belirler.\n",
        "\n",
        "Normalde LoRA sadece weight matrislerini hedef alır. Ama bazı modellerde bias terimleri de önemli olabilir. Bu parametre, bu bias değerlerinin:\n",
        "\n",
        "Eğitilip eğitilmeyeceğini\n",
        "\n",
        "Hangi seviyede eğitileceğini (global mi, katman bazlı mı)\n",
        "\n",
        "belirlememizi sağlar.\n",
        "\n",
        "Kullanım Değerleri ve Anlamları\n",
        "```\n",
        "bias=\"none\"         # Varsayılan. Bias parametreleri eğitilmez. Sadece LoRA ağırlıkları eğitilir, daha hafif model\n",
        "bias=\"all\"          # Tüm bias terimleri eğitilir. Tüm modeldeki bias'lar eğitilir. Daha fazla kontrol, ama daha fazla parametre\n",
        "bias=\"lora_only\"    # Sadece LoRA modülünün içindeki bias'lar eğitilir. LoRA ile ilişkili bias'lar da eğitilir. Dengeli bir seçimdir.\n",
        "```\n",
        "\n",
        "- **Hızlı ve minimum kaynak kullanımı** için: `bias=\"none\"` (varsayılan)\n",
        "- **LoRA modülünde daha hassas öğrenme** isteniyorsa: `bias=\"lora_only\"`\n",
        "- **Tüm modelde detaylı ince ayar** gerekiyorsa (özellikle küçük modellerde): `bias=\"all\"`\n",
        "\n",
        "\n",
        "| **Bias Ayarı**    | **Anlamı**                                                                 |\n",
        "|-------------------|-----------------------------------------------------------------------------|\n",
        "| `\"none\"`          | Hiçbir bias parametresi eğitilmez (varsayılan).                            |\n",
        "| `\"lora_only\"`     | Sadece LoRA katmanlarının içindeki bias parametreleri eğitilir.            |\n",
        "| `\"all\"`           | Modeldeki tüm bias parametreleri eğitilir. Daha fazla hesaplama gerektirir. |\n",
        "\n",
        "\n",
        "| **Kullanım Durumu**                          | **Tercih Edilen Bias Ayarı** |\n",
        "|---------------------------------------------|-------------------------------|\n",
        "| Minimum bellek, hızlı eğitim                | `\"none\"`                     |\n",
        "| LoRA'nın kendi içsel bias öğrenimini istersen | `\"lora_only\"`                |\n",
        "| Daha ince ayar, detaylı müdahale gereken modeller | `\"all\"`                  |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yMnymeLJjpl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### task_type\n",
        "\n",
        "task_type, LoRA’nın uygulanacağı model mimarisini ve görev türünü belirtir. Bu, LoRA’nın hangi katmanlara entegre edileceğini, hangi ayarlarla çalışacağını belirler.\n",
        "\n",
        "Her model ve görev türü için LoRA’nın etkili çalışabilmesi adına farklı hedef modüller gerekebilir. Bu yüzden task_type parametresi, modelin yapısına özel doğru kurulum yapılabilmesi için gereklidir.\n",
        "\n",
        "\n",
        "LoRA bir Transformer modeline entegre edilirken, mimariye göre doğru layer’lara bağlanmalıdır. task_type parametresi PEFT kütüphanesine şu sinyali verir:\n",
        "\n",
        "“Ben causal language model mi eğitiyorum, yoksa bir sınıflandırma modeli mi?“\n",
        "\n",
        "Bu bilgiye göre hangi katmanlara LoRA uygulanacağını otomatik olarak ayarlar.\n",
        "\n",
        " task_type Değerleri ve Açıklamaları\n",
        "\n",
        "| **task_type**           | **Açıklama**                                                            |\n",
        "|-------------------------|-------------------------------------------------------------------------|\n",
        "| \"CAUSAL_LM\"             | Metin üretimi modelleri (LLaMA, GPT, Falcon vb.)                        |\n",
        "| \"SEQ_CLS\"               | Sıralama sınıflandırması (BERT ile duygu analizi)                      |\n",
        "| \"TOKEN_CLS\"             | Token düzeyinde sınıflandırma (adlandırılmış varlık tanıma)             |\n",
        "| \"SEQ_2_SEQ_LM\"          | Encoder-decoder tabanlı modeller (T5, BART, çeviri vb.)                 |\n",
        "| \"QUESTION_ANSWERING\"    | Soru-cevap görevleri (bert-base-uncased + SQuAD gibi)                  |\n",
        "\n",
        "\n",
        "Hatalardan Kaçınmak İçin\n",
        "Modelin mimarisi ile uyumlu bir task_type belirtmezsen, LoRA katmanları yanlış yerlere uygulanabilir veya hiç uygulanmayabilir.\n",
        "\n",
        "Hugging Face model sayfasındaki model_class bilgisi genellikle hangi task_type’ın seçilmesi gerektiğini gösterir.\n",
        "\n",
        "| **Model Türü / Kullanım Senaryosu**          | **Uygun `task_type`** |\n",
        "|---------------------------------------------|------------------------|\n",
        "| GPT-2, LLaMA, Falcon                         | \"CAUSAL_LM\"            |\n",
        "| T5, BART, mT5                                | \"SEQ_2_SEQ_LM\"         |\n",
        "| BERT ile metin sınıflandırma                | \"SEQ_CLS\"              |\n",
        "| BERT ile kelime seviyesinde sınıflandırma   | \"TOKEN_CLS\"            |\n",
        "| Soru-Cevap (BERT + SQuAD gibi datasetler)    | \"QUESTION_ANSWERING\"   |\n"
      ],
      "metadata": {
        "id": "sYs_gAb1jqoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### target_module\n",
        "\n",
        "target_modules, LoRA’nın hangi nöral ağ katmanlarına (modüllerine) entegre edileceğini belirtir. Bu, LoRA'nın ağırlık güncellemelerini hangi katmanlara uygulayacağını belirleyen en önemli hiperparametrelerden biridir.\n",
        "\n",
        "LoRA mimarisinde yalnızca belirli katmanlara \"düşük-rank güncelleme\" (low-rank update) uygulanır. Bu katmanları manuel olarak belirtmek, özelleştirilmiş ve verimli bir ince ayar sağlar.\n",
        "\n",
        "1. LLaMA / GPT Tarzı Modeller (CAUSAL_LM) **kalın metin**\n",
        "\n",
        "`target_modules = [\"q_proj\", \"v_proj\"]`\n",
        "\n",
        "Q ve V projeksiyonları, transformer'da self-attention mekanizmasının kalbidir.\n",
        "\n",
        "Bu iki modül yeterli LoRA etkileşimi sağlar ve oldukça yaygındır.\n",
        "\n",
        "\n",
        "2. BERT Tarzı Modeller (SEQ_CLS / TOKEN_CLS **kalın metin**)\n",
        "\n",
        "`target_modules = [\"query\", \"value\"]`\n",
        "\n",
        "BERT’in attention başlıkları genelde query, key, value olarak adlandırılır.\n",
        "\n",
        "query ve value yeterli olur, key genellikle eklenmez.\n",
        "\n",
        "**3. T5 / BART Gibi Seq2Seq Modeller**\n",
        "\n",
        "`target_modules = [\"q\", \"v\", \"k\", \"o\"]`\n",
        "\n",
        "Bu modellerde farklı adlandırmalar olabilir (q, k, v, o) — modellerin implementation'ına dikkat etmek gerekir.\n",
        "\n",
        "| **Model Türü**     | **task_type**   | **Önerilen `target_modules`**     |\n",
        "|--------------------|-----------------|-----------------------------------|\n",
        "| LLaMA / GPT        | \"CAUSAL_LM\"     | [\"q_proj\", \"v_proj\"]              |\n",
        "| BERT (sınıflama)   | \"SEQ_CLS\"       | [\"query\", \"value\"]                |\n",
        "| BERT (etiketleme)  | \"TOKEN_CLS\"     | [\"query\", \"value\"]                |\n",
        "| T5, BART (çeviri)  | \"SEQ_2_SEQ_LM\"  | [\"q\", \"v\"]                        |\n",
        "\n",
        "\n",
        "| **Hedef**                                   | **target_modules Stratejisi**      |\n",
        "|--------------------------------------------|------------------------------------|\n",
        "| En yaygın LoRA kullanımı (standart setup)  | [\"q_proj\", \"v_proj\"]               |\n",
        "| Daha agresif müdahale                      | [\"q_proj\", \"v_proj\", \"k_proj\"]     |\n",
        "| Daha hafif, hızlı eğitim                   | [\"v_proj\"]                         |\n",
        "| BERT tabanlı ince ayar                     | [\"query\", \"value\"]                 |\n"
      ],
      "metadata": {
        "id": "TG09TI1JkAVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using LoRA Adapters\n",
        "# 🎯 Bu örnekte, müşteri hizmetleri alanında kullanılan bir sohbet botu için\n",
        "# Maykeye/TinyLlama-v0 modeli üzerinde LoRA (Low-Rank Adaptation) kullanılarak\n",
        "# verimli bir ince ayar süreci uygulanmaktadır. Eğitim betiği hazırdır,\n",
        "# amaç yalnızca LoRA yapılandırmasını entegre etmektir.\n",
        "#\n",
        "# 🔧 Uygulama Adımları:\n",
        "# - `peft` kütüphanesinden LoRA yapılandırmasını içe aktarın.\n",
        "# - Varsayılan değerlerle bir `lora_config` oluşturun.\n",
        "# - Bu yapılandırmayı `SFTTrainer` sınıfına dahil ederek eğitimi başlatın."
      ],
      "metadata": {
        "id": "JolyEwSBzb7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import LoRA configuration class\n",
        "from peft import LoraConfig"
      ],
      "metadata": {
        "id": "_AbTVWLR8TDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=12,\n",
        "    lora_alpha=8,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=['q_proj', 'v_proj']\n",
        ")"
      ],
      "metadata": {
        "id": "HsaOTccK8V8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    # Pass the lora_config to trainer\n",
        "    peft_config=lora_config\n",
        ")"
      ],
      "metadata": {
        "id": "gGmin7KI8YvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🧪 Bu çalışmada, sınırlı donanım kaynaklarıyla dil modeli ince ayarı yapılmaktadır.\n",
        "# Geleneksel fine-tuning yerine daha hafif bir yöntem olan LoRA kullanılmıştır.\n",
        "# Model olarak TinyLlama tercih edilmiş, örnek veriler bitext benzeri bir yapıdadır.\n",
        "#\n",
        "# Eğitim kodları büyük ölçüde hazırdır. Bu hücrede yapılması gereken şey:\n",
        "# - LoRA yapılandırmasında rank değerini 2 olarak ayarlamak,\n",
        "# - lora_alpha'yı bu değerin 2 katı (yani 4) yapmak,\n",
        "# - ve görev türünü Llama tipi modellerle uyumlu olacak şekilde \"CAUSAL_LM\" olarak belirtmektir.\n",
        "#\n",
        "# Gerekli değişkenler (model, tokenizer, dataset, training_arguments) önceden tanımlanmıştır.\n"
      ],
      "metadata": {
        "id": "EYZjwLrt-Km7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "YfTlpiLoAB_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    # Set rank parameter\n",
        "    r=2,\n",
        "    # Set scaling factor (2 * r)\n",
        "    lora_alpha=4,\n",
        "    # Set the type of task for LLaMA-style models\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=['q_proj', 'v_proj']\n",
        ")"
      ],
      "metadata": {
        "id": "T93axSijAClS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    max_seq_length=250,\n",
        "    dataset_text_field='conversation',\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config\n",
        ")"
      ],
      "metadata": {
        "id": "zUluLER0AEdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA ve Attention Mekanizmasında q, k, v Yapıları\n",
        "\n",
        "## 1. q, k ve v Nedir?\n",
        "\n",
        "`q`, `k` ve `v` terimleri, transformer modellerinde kullanılan **self-attention** mekanizmasının temel bileşenleridir.  \n",
        "Her biri, giriş verisinin farklı alt uzaylara projeksiyonudur. Bu projeksiyonlar sayesinde model, hangi bilgilere dikkat edeceğini ve ne aktaracağını belirler.\n",
        "\n",
        "- **q (query)**: Sorgu vektörüdür. Modelin hangi bilgiye dikkat etmesi gerektiğini temsil eder.\n",
        "- **k (key)**: Anahtar vektörüdür. Her kelimenin tanımlayıcı bilgisini taşır, sorguya cevap oluşturur.\n",
        "- **v (value)**: Değer vektörüdür. Asıl içeriği taşıyan bileşendir.\n",
        "\n",
        "Bu projeksiyonlar şu şekilde elde edilir:\n",
        "\n",
        "Q = X · W_q\n",
        "\n",
        "K = X · W_k\n",
        "\n",
        "V = X · W_v\n",
        "\n",
        "\n",
        "Attention(Q, K, V) = softmax(QKᵀ / √d_k) · V\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Bu formül, her kelimenin diğer kelimelere ne kadar dikkat etmesi gerektiğini bulur.  \n",
        "- `QKᵀ`: İlgililik skorlarını üretir (query-key benzerliği)\n",
        "- `V`: Asıl bilgi buradan aktarılır\n",
        "\n",
        "---\n",
        "\n",
        "## 3. LoRA Bu Yapılarla Nasıl İlgili?\n",
        "\n",
        "LoRA (Low-Rank Adaptation), büyük dil modellerine **düşük parametreli adaptasyon katmanları** ekleyerek verimli ince ayar (fine-tuning) yapmayı sağlar.  \n",
        "Bu adaptasyon katmanları genellikle `q_proj` ve `v_proj` gibi modüllere uygulanır.\n",
        "\n",
        "### Neden `q_proj` ve `v_proj`?\n",
        "\n",
        "- **q_proj (Query Projeksiyonu)**: Dikkat yönlendirmesini doğrudan etkiler.\n",
        "- **v_proj (Value Projeksiyonu)**: Modelin içerik taşımasını doğrudan etkiler.\n",
        "- Bu iki modül, modelin çıktı üretme kalitesinde belirleyicidir.\n",
        "\n",
        "### Neden `k_proj` Genellikle Dahil Edilmez?\n",
        "\n",
        "- `k_proj` (Key projeksiyonu), sorgularla eşleşme kurallarını belirler.\n",
        "- Bu yapı sabit kalırsa model daha istikrarlı davranır.\n",
        "- Fazla müdahale, dikkat mekanizmasının dengesini bozabilir.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Yaygın target_modules Ayarları\n",
        "\n",
        "Aşağıda farklı model türlerine göre önerilen `target_modules` listesi verilmiştir:\n",
        "\n",
        "| Model Türü         | task_type       | target_modules             |\n",
        "|--------------------|-----------------|----------------------------|\n",
        "| LLaMA, GPT         | \"CAUSAL_LM\"     | ['q_proj', 'v_proj']       |\n",
        "| BERT (sınıflama)   | \"SEQ_CLS\"       | ['query', 'value']         |\n",
        "| BERT (etiketleme)  | \"TOKEN_CLS\"     | ['query', 'value']         |\n",
        "| T5, BART           | \"SEQ_2_SEQ_LM\"  | ['q', 'v']                 |\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Özet\n",
        "\n",
        "- `q`, `k` ve `v`, giriş vektörlerinin öğrenilmiş matrislerle farklı görevlere projekte edilmiş (iz düşürülmüş) halleridir.\n",
        "- Transformer'daki dikkat (attention) işlemleri bu projeksiyonlar üzerinden çalışır.\n",
        "- LoRA, modelin tüm ağırlıklarını değil, yalnızca `q_proj` ve `v_proj` gibi hassas katmanları güncelleyerek verimli adaptasyon sağlar.\n",
        "- Bu sayede daha az parametreyle, daha hızlı ve daha ekonomik fine-tuning mümkündür.\n",
        "\n",
        "---\n",
        "\n",
        "## Kaynaklar\n",
        "\n",
        "- Vaswani et al., 2017. [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "- Hu et al., 2021. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
        "- Hugging Face PEFT Documentation: https://huggingface.co/docs/peft\n",
        "\n"
      ],
      "metadata": {
        "id": "_nMnLTkNAxiO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "guOThFUhBIRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kuantizasyon ile daha küçük model, daha hızlı çıkarım"
      ],
      "metadata": {
        "id": "7noTku7ZPeRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "Model kuantizasyonu, modelleri daha düşük hassasiyetli bir formata dönüştürerek bellek kullanımını azaltır ve çıkarım hızını artırır. Parametreler ve aktivasyonlar daha az bit kullanır; 32 bitlik floatlardan 8 bitlik veya 4 bitlik tam sayılara geçilir, bu da bellek kullanımını azaltır. Kuantizasyon uygulandığında, modelin performansı üzerindeki etkisini en aza indirmek için eğitim süreci ayarlanabilir.\n",
        "\n",
        "Kuantize edilmiş bir model doğrudan ince ayar yapılamaz çünkü ağırlıklar ayrıklaştırılmıştır. Bu sorunu aşmak için daha önce öğrendiğimiz LoRA adaptasyonunu kullanabiliriz. SFTTrainer’ı kullanarak, bitsandbytesconfig ile yüklenen kuantize edilmiş modeli aktarabiliriz ve LoRA yapılandırmasının bir örneğini peft_config olarak peft kütüphanesi aracılığıyla aktarabiliriz. Ardından, modeli her zamanki gibi LoRA adaptasyonu ile trainer.train komutu ile eğitebiliriz."
      ],
      "metadata": {
        "id": "XoiYfSc3PiIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Teknik                           | Ne Yapar?                                                                      | Amaç                                                           |\n",
        "|----------------------------------|--------------------------------------------------------------------------------|----------------------------------------------------------------|\n",
        "| **Ağırlık Kuantizasyonu**        | Ağırlıkları daha düşük bit’li sayılarla temsil eder                            | Modelin boyutunu ve belleğini azaltmak                        |\n",
        "| **Aktivasyon Kuantizasyonu**     | Ara aktivasyonları daha düşük bit ile hesaplar                                 | Anlık RAM kullanımını ve işlem gücünü azaltmak                |\n",
        "| **PTQ (Post-Training Quantization)** | Eğitimden sonra kuantizasyon uygular                                      | Eğitim verisi gerektirmeden model verimliliğini artırmak      |\n",
        "| **LoRA**                         | Tüm ağırlıkları değil, sadece kritik parçaları günceller                        | Az parametreyle fine-tuning yapabilmek                        |\n",
        "| **Distillation (Bilgi Yoğunlaştırma)** | Büyük modelin bilgisini küçük modele aktarır                            | Daha küçük modelle benzer doğruluk sağlamak                   |\n"
      ],
      "metadata": {
        "id": "vLbHyeynPinK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model with quantization\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "3DVSIV7VPjFW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    # set precision (load_in_4_bit, load_in_8_bit)\n",
        "    load_in_4bit=True,\n",
        "    # set quantization type ('fp4' or 4-bit float, 'nf4' or normalized 4-bit float)\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    # set compute dtype (compute precision) (float16, float32, bfloat16)\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ],
      "metadata": {
        "id": "5oSE3ppBPrQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/Llama3-ChatQA-1.5-8B\",\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "id": "fPuQClfyQzH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kuantize edilmiş modeli kullanmak:\n",
        "promptstr = \"\"\"System: You are a helpful chatbot who answers questions about planets.\n",
        "User: Explain the history of Mars.\n",
        "Assistant: \"\"\"\n",
        "inputs = tokenize.encode(promptstr, return_tensors=\"pt\")\n",
        "outputs = model.generate(inputs, max_length=200)\n",
        "decoded_outputs = tokenizer.decode(outputs[0, inputs.shape[1]:], skip_special_tokens=True)\n",
        "print(decoded_outputs)"
      ],
      "metadata": {
        "id": "b0MxIU1pyM_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalde kuantize edilmiş model üzerine fine-tune işlemi yapamayız.\n",
        "# Ancak bu kısıtlamayı Lora ile aşarız\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    peft_config=peft_config,\n",
        "    train_dataset=ds,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    max_seq_length=250,\n",
        "    dataset_text_field='conversation',\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "kBLrgLlf3zc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot için kullanılan modelin GPU bellek kullanımını performanstan çok fazla ödün vermeden azaltmak\n",
        "# Modeli 8-bit kuantizasyonuyla yüklenecek\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "model_name = \"Maykeye/TinyLLama-v0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    # Set 8-bit loading\n",
        "    load_in_8_bit=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    # Set quantization parameters\n",
        "    to load quantized model\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")"
      ],
      "metadata": {
        "id": "6BAynCSX_cMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_name = \"Maykeye/TinyLLama-v0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    quant_type=\"nf4\",                     # Normalized 4-bit quantization\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 # Use bfloat16 for faster inference\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")"
      ],
      "metadata": {
        "id": "0NQBsEao2UGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}